{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce Paradigm  \n",
    "\n",
    "This project is about one of the core strategies in distributed processing: divide and conquer. Word counting from the text of _Alice in Wonderland_ has been used to illustrate the difference between a scalable and non-scalable algorithm, by using Python and Bash scripting. This projects helps understand folowing concepts:\n",
    "* ... the __Bias-Variance tradeoff__ as it applies to Machine Learning.\n",
    "* ... why word counting is considered to be an __\"Embarrassingly Parallel\"__ task.\n",
    "* ... __estimate__ the runtime of embarrassingly parallel tasks using \"back of the envelope\" calculations.\n",
    "* ... __implement__ a Map Reduce algorithm using the Command Line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=8, micro=3, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm you are running Python 3\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder for any data you download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `data': File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "# NOTE: the contents of this directory will be ignored by git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias - Variance\n",
    "__Bias-variance trade off explained below along with what it means to \"decompose\" sources of error and how is this used in machine learning?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> High bias is underfitting, High variance is overfitting(the model might be learning from the noise too) for e.g the regression output will be a curve that would fit data perfectly. \n",
    "To Optimize the error in our model, we need to find the right balance between bias and variance. This is called Bias-Variance Tradeoff. Train the model in such a way that it gives almost the same error on both train and test data.\n",
    "\"decompose\" sources of error is breaking down types of error into reducible and irreducible errors.\n",
    "it is possible to show that the expected test MSE, for a given value x0, can always be decomposed into the sum of three fundamental quantities: \n",
    "\n",
    "1) the variance of $\\hat{f}(x0)$, \n",
    "\n",
    "2) the squared bias of $\\hat{f}(x0)$ and, \n",
    "\n",
    "3) the variance of the error term Ïµ (irreducible error)\n",
    "\n",
    "$ Err(x) = \\big(E[\\hat{f}(x)] - f(x)\\big)^2 + E\\big[\\big(\\hat{f}(x)-E[\\hat{f}(x)]\\big)^2\\big] + \\sigma^2_e $\n",
    "\n",
    "or simply,\n",
    "\n",
    "$ Err(x) = Bias^2 + Variance + Irreducible Error $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n",
    "A common preprocessing step when working with raw files is to 'tokenize' (i.e. extract words from) the text. Within the field of Natural Language Processing a lot of thought goes into what specific tokenizing makes most sense for a given task. For example, choose to remove punctuation or to consider punctuation symbols  'tokens' in their own right. \n",
    "\n",
    "* There can be several scenarios where a tokenizer, if setup in a diferent way will give different results\n",
    "\n",
    "1)If we decide to consider punctuation symbol while extracting words from the text file, then it will lead to a different result as compared to, if punctuation symbols were not considered. \n",
    "\n",
    "2)There can be another case, where upper and lower cases can be tokenized as two different words. \n",
    "\n",
    "3)There can be another scenario where singular and plural words can be a considered as 2 separate tokens or words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C - Fill in the regular expression\n",
    "RE_PATTERN = re.compile(r'[a-z]+')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by', 'the', 'bye', 'what', 'became', 'of', 'alice', 's', 'hats']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - DO NOT MODIFY THIS CELL, just run it as is to check your pattern\n",
    "words = re.findall(RE_PATTERN, \"By-the-bye, what became of Alice's 12 hats?!\".lower())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  170k  100  170k    0     0   149k      0  0:00:01  0:00:01 --:--:--  149k\n"
     ]
    }
   ],
   "source": [
    "# Download Full text \n",
    "# replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "\n",
    "!curl \"https://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the first few lines\n",
    "!head -n 6 data/alice.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jupyter magic command `%%writefile` is a convenient way to develop a habit of creating small files with simulated data for use in developing, debugging and testing code. \n",
    "__The following cells can be run to create a test data file for use in word counting task.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice_test.txt\r\n",
      "alice_test.txt.output\r\n"
     ]
    }
   ],
   "source": [
    "# confirm the file was created in the data directory using a grep command:\n",
    "!ls data | grep test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count in Python\n",
    "\n",
    "Over the course of this term you will also become very familiar writing Python programs that read from standard input and using Linux piping commands to run these programs and save their output to file. __In this question you will write a short python script to perform the Word Count task and then run your script on the _Alice in Wonderland_ text__. You can think of this like a 'baseline implementation' that we'll later compare to the parallelized version of the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - code is available in wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# part b - run it to test your script\n",
    "!python wordCount.py < data/alice_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - run it as is to perform the word count. In the same cell, pipe the output to file\n",
    "\n",
    "!python wordCount.py < data/alice.txt > data/alice_counts.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 10 words & their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"alice\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep alice data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"hatter\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep hatter data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"queen\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep queen data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unix Sorting \n",
    "Another common task is to make strategic use of sorting.     \n",
    "\n",
    "Few important points to note about sorting:\n",
    "\n",
    "* a) the Big O complexity of the fastest comparison based sorting algorithms? \n",
    "\n",
    "* b) the default sorting algorithm in MapReduce is MergeSort algorithm. Big O complexity of this algorithm is: Time complexity is ð‘‚(ð‘›ð‘™ð‘œð‘”ð‘›) and Space complexity is ð‘‚(ð‘›) \n",
    "\n",
    "* c) the reason why this algorithm was chosen is because MapReduce implements sorting algorithm to automatically sort the output key-value pairs from the mapper by their keys. One cannot change the MapReduce sorting method, the reason is that data comes from the different nodes to a single point, so the best algorithm that can be used here is the merge sort\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3006 data/alice_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# unix command to check how many records are in word count file\n",
    "!wc -l data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unix command to sort word counts alphabetically \n",
    "!sort data/alice_counts.txt > data/alice_counts_A-Z.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t695\n",
      "abide\t2\n",
      "able\t1\n",
      "about\t102\n",
      "above\t3\n",
      "absence\t1\n",
      "absurd\t2\n",
      "accept\t1\n",
      "acceptance\t1\n",
      "accepted\t2\n"
     ]
    }
   ],
   "source": [
    "# run to confirm if sort worked\n",
    "!head data/alice_counts_A-Z.txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unix command to sort word counts from highest to lowest count\n",
    "!sort -nr -k 2 data/alice_counts.txt > data/alice_counts_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t1839\n",
      "and\t942\n",
      "to\t811\n",
      "a\t695\n",
      "of\t638\n",
      "it\t610\n",
      "she\t553\n",
      "i\t546\n",
      "you\t486\n",
      "said\t462\n"
     ]
    }
   ],
   "source": [
    "# part e - run to confirm if sort worked\n",
    "!head data/alice_counts_sorted.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Word Count (part 1)\n",
    "\n",
    "What would happen if we tried to run our script on a much larger dataset? For one thing, it would take longer to run. However there is a second concern. The Python object that aggregates our counts (`defaultdict`) exists in memory on the machine running this notebook. If the vocabulary is too large for the memory space available, it would crash the notebook. The solution? Divide and Conquer! Instead of running the script on the whole dataset at once, we could split our text up in to smaller 'chunks' and process them independently of each other. \n",
    "> __In this exercise we will use a bash script to \"parallelize\" Word Count.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure your scripts are executable\n",
    "!chmod a+x pWordCount_v1.sh\n",
    "!chmod a+x wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bash script `pWordCount_v1.sh` takes an input file, splits it into a specified number of 'chunks', and then applies an executable of your choice to each chunk. \n",
    "# parallel word count on Alice text on 4 parallel processes. Results redirected into a file called `alice_pCounts.txt.\n",
    "\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' 'wordCount.py' > 'data/alice_pCounts.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> The count for word 'alice' does not come as expected. Here, each of the mappers are outputting the count of their chunk.\n",
    "In order to fix the problem, we need to add the reducer that will aggregate the results from the mappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t113\n",
      "alice\t126\n",
      "alice\t122\n",
      "alice\t42\n"
     ]
    }
   ],
   "source": [
    "# check alice count \n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the aggregateCounts script is executable\n",
    "!chmod a+x aggregateCounts_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The python script, `aggregateCounts_v1.py`, reads word counts from standard input and combines any duplicates it encounters. In `pWordCount_v1.sh`, a one-line modification has been made so that it accepts `aggregateCounts_v1.py` as a 4th argument and uses this script to combine the chunk-ed word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel word count on Alice text\n",
    "!./pWordCount_v1.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v1.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\r\n"
     ]
    }
   ],
   "source": [
    "# check alice count\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !man sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Word Count (part 2)\n",
    "\n",
    "From here on out, we'll refer to the two python scripts you passed to `pWordCount_v1.sh` as '_mapper_' and '_reducer_'. The bash script itself served as our '_framework_' -- it split up the original input, then ___mapped___ our word counting script on to each chunk, then ___aggregated (a.k.a. reduced)___ the resulting files by piping them into our collation script.  Unfortunately, there is a major scalability concern with this particular implementation. __In this exercise we'll fix our implementation of parallel word count so that it will be scalable.__\n",
    "\n",
    "__Note:__ MapReduce uses the Merge-Sort algorithm under the hood. Linux `sort` command has a merge option which you can use to simulate the MapReduce framework. \n",
    "\n",
    "\n",
    "* __a) The potential scalability problem with the provided implementation of `aggregateCounts_v1.py` is that the Python object that aggregates our counts (defaultdict) exists in memory on the machine running this notebook. If the vocabulary is too large for the memory space available we would crash the notebook.\n",
    "\n",
    "\n",
    "* __b) Fortunately, a 'strategic sort' can solve this problem. The code for this is available in `pWordCount_v2.sh` file. The code makes changes that alphabetically sort the output from the mappers (`countfiles`) before piping them into the reducer script.\n",
    "\n",
    "\n",
    "* __c) The script `aggregateCounts_v2.py` takes advantage of the sorted input to add duplicate counts without storing the whole vocabulary in memory. \n",
    "\n",
    "\n",
    "* __d) This rewritten reducer sets us up for a truly scalable solution, but doesn't get us all the way there. In particular, while we chunked our data so it can be processed by multiple mappers, we're still streaming the entire dataset through one reduce script. If the vocabulary is too large to fit on a single computer, we might split the word counts in half after sorting them, then perform the reducing on two separate machines. In case, we split the word counts in half after sorting them, and then perform the reducing on two separate machines, there can be issue of duplicate words. Same set of words can exist in both the files, that can lead to miscount of words once all the words are put together at the end.\n",
    "\n",
    "\n",
    "* __e) We need to come up with a different way of splitting up the data that would allow us to perform the reducing on separate machines without needing any postprocessing? Put a check in place for separation while splitting up the data on to two machines. If a word exists in reducer 1, add the second matching word to the same reducer 1. If it doesn't exist, then send that new word to the second reducer. In this way, we can ensure that the one word group won't be separated into two different reducers and thereby eliminating the duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add permissions to your new files \n",
    "!chmod a+x pWordCount_v2.sh\n",
    "!chmod a+x aggregateCounts_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t2\n",
      "file\t3\n",
      "for\t1\n",
      "has\t1\n",
      "is\t2\n",
      "lines\t1\n",
      "small\t3\n",
      "test\t3\n",
      "this\t3\n",
      "two\t1\n"
     ]
    }
   ],
   "source": [
    "# testing code on the test file\n",
    "!./pWordCount_v2.sh 4 'data/alice_test.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running code on the Alice file \n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'aggregateCounts_v2.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\n"
     ]
    }
   ],
   "source": [
    "# confirm that 'alice' count is correct\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Scalability Considerations\n",
    "\n",
    "Some Big ideas that underlie large scale processing: __scale \"out,\" not \"up\"; assume failures are common; move processing to the data; process data sequentially and avoid random access; hide system-level details from the application developer; and seamless scalability.__\n",
    "\n",
    "\n",
    "> __a)__ From a scalability perspective, the two features of an \"ideal algorithm\" are: \n",
    "\n",
    "    1) In terms of data: given twice the amount of data, the same algorithm should take at most twice as long to run, all else being equal\n",
    "\n",
    "    2) In terms of resources: given a cluster twice the size, the same algorithm should take no more than half as long to run.\n",
    "\n",
    "> __b)__ with simple algorithm, the ideal algorithm criteria matches. With increase in the number of partition, the execution time cuts down in half pretty much. \n",
    "\n",
    "> __c)__ In case of word count algorithm, Ideal algorithm criteria does not hold true. Increasing our partitions does not improve the runtime, in fact, at many partitions, it gets worse. One reason for this can be the bottle neck in merging of the partitions before being given to the reducer. Another reason can be the single reducer slowing down the computation. The communication cost can also play a major role here, as we see that with increase in the number of partitions beyond 8, the communication cost is starting to take over and dominate the total time.\n",
    "\n",
    "> __d)__ Mapper runs faster, sorting slows things down. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to create the mapper referenced in `part b`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `demo': File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This mapper reads from STDIN and waits 0.001 seconds per line.\n",
    "Its only purpose is to demonstrate one of the scalability ideas.\n",
    "\"\"\"\n",
    "import sys\n",
    "import time\n",
    "for line in sys.stdin:\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the mapper is executable\n",
    "!chmod a+x demo/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the next three cells to apply our demo mapper with 1, 2 and 4 partitions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.83 s Â± 6.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 s Â± 5.45 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74 s Â± 4.54 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to repeat this process with your word count algorithm.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758 ms Â± 3.42 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 1 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 ms Â± 7.93 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 2 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764 ms Â± 4.6 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 4 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824 ms Â± 3.64 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 8 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926 ms Â± 5.59 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 16 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12 s Â± 9.21 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount_v2.sh 32 'data/alice.txt' 'wordCount.py' 'aggregateCounts_v2.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embarrassingly Parallel\n",
    "\n",
    "\n",
    "> __a)__ In parallel computing, an embarrassingly parallel workload or problem is one where little or no effort is needed to separate the problem into a number of parallel tasks. This is often the case where there is little or no dependency or need for communication between those parallel tasks, or for results between them. For example, in word counting, lines from the document can be easily split and distributed across hundreds of machines because there are no inherent dependencies between two lines.\n",
    "This term describes implementation of task.\n",
    "\n",
    "> __b)__ For any function that is associative commutative, the algorithm becomes an Embarrassingly Parallel one, because of the inherent properties associated with being associative and commutative.\n",
    "A commutative problem is the one where $(a+b)=(b+a)$. An associated problem is one where $(a+b)+c=a+(b+c)$. These two properties guarantee an Embarrassingly Parallel operation since the problem can be broken down and put together in different order with no impact to the ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "297px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "951px",
    "left": "0px",
    "right": "1561px",
    "top": "106px",
    "width": "600px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
