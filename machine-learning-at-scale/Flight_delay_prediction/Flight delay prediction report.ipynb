{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e47e9bac-dc51-4933-abeb-288664d7c124",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Flight Delay Predictions - Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d783ebf-14c6-471e-96a3-e091b16cb2b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Abstract\n",
    "\n",
    "Flight delays are a problem in the United states as we rely heavily on air travel. This puts a significant strain on US air traffic control costing airlines industry billions of dollars every year and causing inconveniece and additional costs to passengers. Various studies conducted on the impact of flight delays conclude that increased flight delays are related to increased costs. \n",
    "\n",
    "Current airlines have too many delays and not enough lead time on notifications when delays inevitably happen. In this project, analysis is done on flight departure and arrival information along with weather data from 2015-2019 to create Machine Learning models that can predict flight departure delay two hours prior to the flight's scheduled departure time. Used F1-score of 0.6 to assess the performance of the classifier since it provides a balance between airlines and passenger needs. Logistic Regression, Random Forest, GBT, XGBoost, and SVM models were explored. Afterwards, it was decided to fine tune the faster model, Logistic Regression, and the most precise algorithm based on the metric of choice, in this case XGBoost which achieved an F1 score of 0.53."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "683d4c13-182d-4f4c-93de-0d6d7833a480",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 1 - Introduction & Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e0854286-849b-41e7-9baa-6a16fb14e43c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.1 - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3db804b7-3fbd-4e37-98b6-ffda5c32bd79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src ='https://s6.gifyu.com/images/departing.gif' width=\"500\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fcb0ff36-7fa6-4644-8981-c7205d614da6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the recent past, we have seen many instances where several airlines had thousands of flights delays and cancellations, as the carriers struggle to recover from disruptions caused by severe weather and staffing shortages.\n",
    "\n",
    "According to the Federal Aviation Administration, there can be several [reasons](https://aspmhelp.faa.gov/index/Types_of_Delay.html) for Airline delays. These can be categorized as follows:\n",
    "\n",
    "**Carrier Delay:** Delay that is within the control of the airline carrier. <br>\n",
    "**Late Arrival Delay:** Arrival delay at an airport due to the late arrival of the same aircraft from previous airport (*Delay propagation*).<br>\n",
    "**NAS Delay:** Delay that is within the control of the National Airspace System (NAS).<br>\n",
    "**Security Delay:** Evacuation of a terminal, security breach, long lines in excess of 29 minutes at screening areas.<br>\n",
    "**Weather Delay:** Extreme or hazardous weather conditions that are forecasted or manifest themselves before or during flight.<br>\n",
    "**OPSNET Delay:** Delays due to Instrument Flight Rules (IFR) traffic of 15 minutes or more, experienced by individual flights.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86229d05-ce04-48b2-87cf-d756aefb48e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Flight delays are estimated to have cost air travelers billions of dollars (assuming an average value of passenger’s time of $47). Also, the delays have a negative impact in the airline industry, the delays cost several billion dollars in additional expense. In 2018, FAA/Nextor estimated the annual cost of delays to be $28 billion. These costs include: direct cost to airlines and passengers, lost demand, and indirect costs.\n",
    "\n",
    "**Causes of flight delays**\n",
    "\n",
    "The Federal Aviation Administration (FAA) indicates that the largest cause of air traffic delays (delay by cause) between 2008 and 2013 is weather (69%), followed by volume - caused by too much demand (19%), runway unavailability (6%), other causes (5%), and equipment (1%). To put this into perspective, in 2013 the 69% represented approximately 10 million minutes (6945 days). As previously mentioned, the delay affects airlines and passengers. Currently (2021), the cost to the air carrier operators for an hour of delay ranges from about $1,400 to $4,500, and the value for the passenger time increases the cost another $35 per hour for personal travel or $63 if the travel is related to business for every person on board. Delayed and canceled flights cause inconvenience for the passengers in addition to loss of trust between the passengers and the airline.\n",
    "\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/delay.jpg'> \n",
    "<br>\n",
    "\n",
    "The FAA has also studied airports with that have the worst weather-related delays. New York area, which combines Newark, LaGuardia, and Kennedy airports is the highest in the country, followed by Chicago, Philadelphia, San Francisco, and Atlanta. The number of delays are 57,000, 26,000, 18,000, 16,000, and 12,000 respectively, as we can see in the graph below.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/top-weather-delay-airports.jpg'>\n",
    "<br>\n",
    "\n",
    "These seven airports with the worst weather-related delay experience many impacting weather events but weather alone does not necessarily lead to huge delays. This is because many delayed planes can be shifted to non-weather periods without overloading the system.\n",
    "\n",
    "The issue araises if an airport has much excess capacity, and airports with the most weather delays also tend to operate close to capacity for large parts of the day. System-impacting weather, combined with excess demand, means that delayed flights may have to wait hours to land or depart.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/weather-type.png'>\n",
    "<br>\n",
    "\n",
    "The goal of this project is to use airline information and weather data to predict flight delays two hours before the scheduled departure time. We define flight delay as flight departing fifteen minutes or more after the scheduled departure time or cancellation of the flight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "31327031-b845-4c3c-a979-aa1687b5aefd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.2 - State-of-the-art research\n",
    "\n",
    "There have been a number of successful studies conducted in the past to predict flight delays caused by weather. Study by *Yazdi et al., 2020* [9] achieved an accuracy of the model on balanced dataset 96.2%.  \n",
    "\n",
    "Ongoing research on this subject commonly utilize Gradient Boosting Classifiers [3], random forest [4], AdaBoost and k-Nearest Neighbors [5], for flight departure and arrival delays. Chakrabarty [3] used a Grid Search to optimize the hyperparameters for Gradient Boosting Classifier Model and achieved a validation accuracy of 85.73% in a balanced dataset. A Convolutional neural network model created by Jiang et al. [6] predicted multi-class flight delay with 89.32% prediction accuracy.\n",
    "\n",
    "Other studies for example Huo et al., 2021 [10] revealed that random forest models had the best outcomes compared to other (logistic regression, KNN, decision trees, and Naive Bayes) models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9c377409-13a5-4fce-86eb-2afbc99e2199",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.3 - Evaluation Metrics\n",
    "\n",
    "The flight delay problem can be categorized as a classification problem, with the *negative* and *positive* labels defined as *no-delay* and *delay*, respectively. From a business standpoint, both false positives (FP), i.e. predicting no-delay flight as delayed, and false negatives (FN), i.e. predicting a delayed flight as without delay, can have adverse effects on passenger travel experience and airline profitability. The metrics to evaluate the performance of such a classifier should not only fit better to the data with severe class imbalance, but also be able to keep a good balance between the number of FP and FN predictions.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curves are widely used to evaluate binary classifiers. However, they may not be the best choice in terms of reliability when the problem of class imbalance is associated to the presence of a low sample size of minority instances, since this may provide an excessively optimistic view of the performance. The tradeoff between precision and recall, on the other hand, make it possible to better assess the performance of a classifier on the minority class.\n",
    "\n",
    "Accuracy is one of the most used evaluation indexes in classification. However, just as ROC, is not the best choice with imbalance datasets. To improve accuracy, the model tends to identify the minority samples as the majority, and the model can obtain higher accuracy, but the prediction of delayed samples is almost ineffective. For this reason, the predicted results should be evaluated by Precision, Recall, and F1 Score in the classification problem.\n",
    "\n",
    "Precision is defined as the number of correct positive predictions made: \n",
    "\n",
    "$$ Precision=\\frac{TP}{TP+FP} $$\n",
    "\n",
    "Recall quantifies the number of correct positive predictions made out of all positive predictions that could have been made: \n",
    "$$ Recall=\\frac{TP}{TP+FN} $$\n",
    "\n",
    "F1-score is a universal metric that has been used by many ML experts and is a representation of the balance between precision and recall. \n",
    "\n",
    "F1-score is defined as : \n",
    "\n",
    "$$ F1_{score}=\\frac{2 \\times precision \\times recall}{precision+recall} $$\n",
    "\n",
    "Thus, it will be used as the main metrics to assess the performance of our classifier. The target goal is to have a minimum F1-score of 0.6. In addition, to compare between different classifiers and also to other studies, we also consider accuracy and area under the ROC curve as a secondary metrics.\n",
    "\n",
    "Due to the limited data (2015-2019), scope and time for this project, we believe that a F1 score of 60% would be a good indicator of successful prediction of flight delays. F1 scores provide a balance between precision and recall, which translates to a balance between airlines and passengers needs. Flight delay without warning (low recall) causes inconvenience for passengers.  If a flight is predicted as a delay but ends up being on time (low precision), passengers may miss their flights and this can impact revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c317247f-a3e1-481d-84ab-502e02930beb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.4 - Research Question\n",
    "\n",
    "To provide a well-informed customer experience, in this project we (the airline carrier) would like to answer the following specific question:\n",
    "\n",
    "**Can we predict flight delays of 15 minutes or more and flight cancellations, two hours prior to the scheduled flight departure time with a F1 score of over 60% using previous years airline and weather data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4bc96fa4-b1fa-45fe-a896-992f5497d411",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.5 - Datasets\n",
    "\n",
    "\n",
    "We use four datasets, three (airlines, weather and stations) of which were provided for the project, and one which we sourced separately. The airline data originally came from the US Department of Transportation's passenger flight's on-time performance data. In our case, it was subsetted down to only US domestic flights from the year 2015-2019. It was already set up with the feature \"dep_del_15\", which indicates whether a flight was more than 15 minutes late, but not if the flight was canceled. The weather and station data was from the National Oceanic and Atmospheric Administration repository, with data from stations all around the world. \n",
    "\n",
    "The fourth dataset 'Airport, airline and route data' was sourced from https://openflights.org/data.html. This provided us with the additional airport features (airport codes & timezone) that we needed to easily join our data. These four datasets were joined together to create one massive dataset with all flight and weather features on US domestic flights from 2015-2019. From there, we created additional features from the data such as percent and mean delays across different aggregations, holiday indicators, prior delays, delay potential, and others.\n",
    "\n",
    "*Because we’re working with time series data, we can't just split the data without thought, since time may have a factor in whether flights are delayed. Instead, we used cross validation to split into our train and test sets. We have significantly more on-time flights compared to delayed flights, which makes our data set unbalanced. In at least one previous study, upsampling the delayed data was shown to overfit Yazdi et al., 2020 [9] so we undersampled our on-time flights to balance the two classes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb64d118-cb47-42e2-9074-4d364944bd34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1.6 - Models\n",
    "\n",
    "Based on State-of-the-art research, the following four models were selected to predict flight delays:\n",
    "<br>\n",
    "- Logistic Regression (baseline),\n",
    "- Random Forest, \n",
    "- Gradient Boost Tree, \n",
    "- Support Vector Machine,\n",
    "- XGBoost\n",
    "\n",
    "We expected our random forest model to give us the best results based on previous studies Huo et al., 2021 [10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df2c2ea1-e143-4723-9df3-7016fec5da91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 2 - EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63a93443-8753-4b54-afd2-7328a36fda31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1 - Objective\n",
    "\n",
    "Exploratory data analysis (EDA) is used widely used by data scientists to analyze and investigate datasets and summarize their main characteristics. Some of the goals of EDA are: have a basic understanding of the general structure and size of the data, have a basic understanding and extent of missing data and the impact, discover outliers and potential erroneous data, understand distributions of the variables of interest, identify potential relationships between the candidate predictors and the outcome variable (departure delays of over 15 minutes `DEP_DEL_15` that was later on transformed to `departure_delay_boolean`), remove highly correlated features, identify important features variables to include in the model to reduce processing cost and improve scalability, and infer possible approaches to feature engineering.\n",
    "\n",
    "The steps taken for our EDA are:<br>\n",
    "\n",
    "- Setup Environment - Setup Databricks notebook environment, import relevant libraries and setup Azure blob storage access\n",
    "- Load Data - Load data from class Azure blob storage for Airline, Weather and Station datasets. Load Airport data from external datasource.\n",
    "- Data Processing & EDA\n",
    "- Define Helper Functions to address missing values, get data statistics (numerical, categorical)\n",
    "  \n",
    "In the first stage of the project, the EDA was done using a smaller dataset (a 3 month dataset). The dataset was transformed to a Pandas dataframe to take advantage of the flexibility that Pandas and Matplotlib packages offer. Pandas DataFrame loads all the data into memory on a single machine for rapid execution. It's extemely powerful in manipulating data. The EDA for the 3 month dataset can be found in the [Exploratory Data Analysis Notebook for the 3 month dataset](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/2297543790294726/command/2297543790294727)\n",
    "\n",
    "Unfortunately, during the second stage of our project when the full dataset was used, Pandas was not able to handle the amount of data of the full weather and airline datasets (scalability issues with Pandas). The 3 month EDA provided the necessary insights to perform the full join of the datasets. Once we had the final joined dataset, we preformed EDA using pyspark. The EDA implementation for the full dataset can be found in the [Exploratory Data Analysis Notebook for the full dataset](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/410528237318226/command/671119174921728). \n",
    "\n",
    "Five helper functions were created:<br>\n",
    "- missing_values(): Helper function to count number of nulls and nans in each column\n",
    "- get_data_stats_numerical(): Function to get stats for numerical variables\n",
    "- get_data_stats_categorical(): Function to get stats for categorical variables\n",
    "- get_data_stats(): Function to get stats for all variables\n",
    "- handleMissingValues(): A function to handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ee43070-64d0-4f0c-b810-794c59e20505",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2 - DataSets\n",
    "\n",
    "The datasets provided for the project are Airline (Flight information), Weather, Stations (Weather Stations). In addition, we had to use an external dataset for Airport information. These are explained in the sections below.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/datasets.png' alt=\"Drawing\" width=\"400\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "571ff2d1-0f86-4693-ab5b-fca210fd81ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.1 - Airline DataSet\n",
    "\n",
    "During the [first EDA stage](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/2297543790294726/command/2297543790294727), we were able to have a basic understanding of the record counts, general structure/schema, data characteristics, and relevant features for this 3 month dataset. This dataset has a total of 161057 records and a total of 109 features. The full dataset has a total 63493682 records, 109 columns. We explored the DEP_DELAY_NEW data distribution, the relationship between delay/distance features, and identified the counts for delayed and on-time flights. The figure below show the Histogram of the DEP_DELAY_NEW and the relationship between Distance and Departure Delays:\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/EDA3m-fig1.jpg' alt=\"Drawing\" width=\"700\"/>\n",
    "<br>\n",
    "\n",
    "The figure on the left shows a right-skewed data distribution, indicating that shorter delays are more frequent. The figure on the right, indicates that the mayority of the delays ocurr when the flights are less than 23,000 miles. \n",
    "\n",
    "We also identified the data distribution of the delayed vs on-time flights. About 23% of the flights get delayed, compared to at 74% of flights arriving on time, leading to an imbalance dataset typical of classification problems. \n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/EDA3m-fig2.jpg' alt=\"Drawing\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "On the full dataset, the number of flight that get delayed decreased to 18%, and the number of flights arriving on time increased to 81% (code can be found in the [Exploratory Data Analysis Notebook for the full dataset](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/410528237318226/command/671119174921728). \n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/EDA-fig3.jpg' alt=\"Drawing\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "As part of this stage, we identify and handle nulls/missing data, filtering features with Null values > 5%. After filtering the data, we reduced the dataset to 53 features (53 out of 109 original features). \n",
    "\n",
    "Once we had the relavant features selected, worked on data preparation. Some of the task done during the data preparation are listed below:\n",
    "- Removed features with missing % > 5\n",
    "- Removed duplicate records\n",
    "- Removed records with target variable (DEP_DEL15) is Null\n",
    "- Added binary categorical variable (outcome) DEP_DELAYED, that is 1 if CANCELLED or DEP_DEL15 is 1\n",
    "- Added DIV_DELAY_ARR feature\n",
    "- Updated CRS_DEP_TIME to be a 4 digit value and extract minutes into MINUTES\n",
    "- Updated Flight data FL_DATE to TIMESTAMP (yyyy-MM-dd HHmm)\n",
    "- Modified TIMESTAMP to reflect time when weather data is posted (51 mins after the hour) using NEW_TIMESTAMP\n",
    "- Added HOURLY_TIMESTAMP, PREV_HOURLY_TIMESTAMP features\n",
    "\n",
    "We also added a **HOLIDAY feature**. Travel during the holidays can have a negative effect on travel delays, for this reason we created a Holiday feature to capture the holiday days, in addition we included a buffer of 1 day before and after the holiday in this feature, since people then to travel before the actual holiday. Also, Mondays and Fridays are configured as holiday days, since delays occur with more frequency during those days.\n",
    "\n",
    "**Flight delays due to late arrival of aircraft from a previous flight:**\n",
    "Flight delays can have a snowball effect on other flights. For this reason, we decided to incorporate a pev_dep_delayed_confirmed (previous departure delayed confirmed) feature. To achive this task, a window funcion was used based on the tail number and planned arrival time to previous flight information, such as PLANNED_PREV_ARR, PREV_DEP_DELAYED, PREV_DEP_TIME, and PREV_ORIGIN features. Some addtional features and conversions were done to achive this task, for example capturing the schedule departure and arrival time of each flight, and converting local times to UTC time. The actual incorporation of the pev_dep_delayed_confirmed feature was done in Section 4.2 after the AirportStation - Airline datasets were joined.\n",
    "\n",
    "Some additional exploration was done to understand the \"Distribution of Flights by Departure Time\", \"Distribution of Departure Times by Proportion of Flights Delayed\", \"Distribution of Airlines by Proportion of Flights Delayed\", and \"Histogram of Flight Delays (By Minutes)\". Some key information capture here was that the distribution of Departure time by Proportion of Flight delayed have an increasing trend as the day progresses, and that the top three airlines with most delays correspond with Frontier (F9), United Airlines (UA), and Spirit Airline (NK).\n",
    "\n",
    "In addition, some temporary dataframes were generate to capture Route delay, Origin delay, and Destination delay intermidiate features to be used after the joining the datasets.\n",
    "\n",
    "Data exploration for the full dataset was done after joining all datasets using PySpark (see section 2.6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcaff20f-f0fc-4df9-844b-fbe1502ca026",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.2 - Airport DataSet\n",
    "\n",
    "During the EDA on this dataset, were were able to have basic understanding of the counts, general structure/schema, data characteristics. This full dataset had a total of 7698 records, 14 columns.\n",
    "Basic data exploration was done to understand relevant features that could help us join the airport and station datasets. This dataset provided the key information of the airport codes (iata and icao), timezone that was used for local time conversion to UTC.    \n",
    "\n",
    "During our initial data exploration using the 3 month dataset, we were able to explore the data distribution for the departure delay >= 15 minutes.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda_3m_c127.png' alt=\"Drawing\" width=\"800\"/>\n",
    "<br>\n",
    "\n",
    "We can observe a similar data distribution in both airports Atlanta (ATL) and Orlando (ORD). With more delays in Chicago (ORD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3e85fcd5-5dfc-41a8-8bad-50e42045f589",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.3 - Station DataSet\n",
    "\n",
    "The EDA provided a basic understanding of the counts, general structure/schema, data characteristics, This full dataset had 5,004,169 records and 12 columns.\n",
    "Data was explored, and relevant features were identified for futher data processing during the join. \n",
    "Feature Selection:\n",
    "- The column `neighbor_call`and `neighbor_id` from the Station dataset is used to join with Airport dataset on the column `neighbor_call` = `ICAO` \n",
    "- The column `neighbor_id` from the Station dataset is used to join the Weather dataset on the column `neighbor_id` = `STATION` \n",
    "As part of the data processing, we filtered distinct values to create a data dictionary of a list of all the airport stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d2f5876c-dfd9-433c-9e95-88c745a0427a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 2.2.4 - Weather DataSet\n",
    "\n",
    "During the EDA, we were able to have a basic understanding of the counts, general structure/schema, data characteristics. This 3 month dataset has a total of 29823926 records and a total of 177 features. The full dataset has a total 630904436 records, 177 columns. We explored the data and identified relevant features, such as \"STATION\", \"DATE\", \"REPORT_TYPE\", \"LATITUDE\", \"LONGITUDE\", \"NAME\", \"WND\", \"CIG\", \"VIS\", \"TMP\", \"DEW\", and \"SLP\".\n",
    "\n",
    "We filtered the data by REPORT_TYPE = FM_15 because other types have mostly null values, without much data. FM-15 reports correspond with \"Aviation routine weather report\".\n",
    "\n",
    "Some of the data preparation done to this dataset corresponds with:\n",
    "- Splited WND, CIG, VIS, TMP, DEW, SLP and add aliases for split data\n",
    "- Identified missing values and replaced by None\n",
    "- Added column hourly_timestamp_w for TIMESTAMP data without minute information. This will be used for join.\n",
    "\n",
    "The data distribution for the weather conditions WND, CIG, VIS, TMP, DEW, SLP can be seen below.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda_3m_c120.png' alt=\"Drawing\" width=\"1500\"/>\n",
    "<br>\n",
    "\n",
    "\n",
    "Data exploration for the full dataset was done after joining all datasets using PySpark (see section 2.6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe00dd7a-28a1-4304-9fb3-4e09e477d16c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.3 - Join : Overview\n",
    "\n",
    "We will now join the datasets, starting with Airport and Station data, then the resulting dataset with Airlines, followed by joining with the Weather dataset.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/join_overview.png' alt=\"Drawing\" width=\"900\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2737c13-7cb0-4d77-81ad-d84220262ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.3 - Join : Airport - Stations\n",
    " \n",
    "We joined the Airport and Stations datasets to get AirportStation dataset using the following line of code:\n",
    "  `df_join_airportcode_station = df_airport_codes.join(df_stations_clean,df_airport_codes.icao == df_stations_clean.neighbor_call, \"inner\")`\n",
    "\n",
    "The join was based on columns df_airport_codes.icao == df_stations_clean.neighbor_call, with an \"inner\" join.\n",
    "\n",
    "Once the dataset was joined, we filtered distinct values, and selected relevant features (iata, icao, DST, timezone, neighbor_name, neighbor_id) and saved the dataset to our blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30ae1f86-051f-4a78-8d4e-f216963e54c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.4 - Join : AirportStation - Airline\n",
    "\n",
    "We joined the AirportStation dataset with the Airline datasets to get AirportStationAirline dataset using the following line of code:\n",
    "`df_join_ac_st_airline = df_airlines_eda.join(df_join_airportcode_station,df_join_airportcode_station.iata == df_airlines_eda.ORIGIN,\"left\").cache()`\n",
    " \n",
    "The join was based on columns df_join_airportcode_station.iata == df_airlines_eda.ORIGIN, with a \"left\" join.\n",
    "\n",
    "Once the dataset was joined, we changed the change HOURLY_TIMESTAMP & PREV_HOURLY_TIMESTAMP to UTC TIMESTAMP. \n",
    "\n",
    "In addition, we created a new feature: `planned_time_between_flights` that will be later on use to account for connections that have little time in between, see section 2.6 for more details. \n",
    "\n",
    "Relevant features were selected and saved to our blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e37783ba-957a-4c56-a45e-e9af7b320533",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.5 - Join : AirportStationAirline - Weather\n",
    "\n",
    "We joined the AirportStationAirline dataset with the Weather dataset to get AirportStationAirlineWeather dataset. We joined the weather dataset twice. The first join was based on origin/station columns and utc_hourly_timestamp/hourly_timestamp_w. The second join was based based on origin/station columns and utc_prev_hourly_timestamp/hourly_timestamp_w. The first join provided weather conditions right before our prediction. An the second join, the second previous weather conditions before our prediction.\n",
    "\n",
    "Code for the first join:\n",
    "`df_join_ac_st_weather = df_join_ac_st_airline_4.join(df_weather_clean, (df_join_ac_st_airline_4.neighbor_id_origin == df_weather_clean.STATION) & (df_join_ac_st_airline_4.utc_hourly_timestamp == df_weather_clean.hourly_timestamp_w),\"left\")`\n",
    "\n",
    "Once joined, we selected relevant features and generated the df_join_ac_st_weather_clean dataset to be used in our second join.\n",
    "\n",
    "Code for the second join:\n",
    "`df_join_ac_st_weather_clean_2 = df_join_ac_st_weather_clean.join(df_weather_clean, (df_join_ac_st_weather_clean.weather_station == df_weather_clean.STATION) & (df_join_ac_st_weather_clean.utc_prev_hourly_timestamp == df_weather_clean.hourly_timestamp_w),\"left\")`\n",
    "\n",
    "Once the join was completed, we selected relevant features, and saved the data to our blob storage.\n",
    "\n",
    "\n",
    "Some visualization were done post-join\n",
    "- Departure Delays by Month\n",
    "- Departure delays by Carrier, Origin, Month and Day of Week\n",
    "- Departure delays by Weekday and Hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "001dbea7-7ca4-4ce4-a6ad-20ac26d6c1d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.6 - Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "475031b9-e6dd-4b7a-9da1-0d871033b3f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As mentioned in section 2.2.1 and section 2.4, three features `holiday`, `prev_dep_delayed_confirmed`, and `planned_time_between_flights` were created.\n",
    "\n",
    "Travel during the holidays can have a negative effect on travel delays, for this reason we created a `holiday` feature to capture the holiday days, in addition we included a buffer of 1 day before and after the holiday in this feature, since people then to travel before the actual holiday. Also, Mondays and Fridays are configured as holiday days, since delays occur with more frequency during those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd57606f-1de5-4187-ba00-debe7e104b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_airlines_eda = df_airlines_eda.withColumn('HOLIDAY', expr(\"\"\"CASE WHEN FL_DATE in ('2015-01-01', '2015-01-19', '2015-02-16', '2015-05-25', '2015-07-04', '2015-09-07', '2015-10-12' ,'2015-11-11', '2015-11-26', '2015-12-25',\n",
    "'2016-01-01', '2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04', '2016-09-05', '2016-10-10',\n",
    "'2016-11-11', '2016-11-24', '2016-12-25', '2017-01-02', '2017-01-16', '2017-02-20', '2017-05-29', \n",
    "'2017-07-04', '2017-09-04', '2017-10-09', '2017-11-11', '2017-11-23', '2017-12-25', '2018-01-02', \n",
    "'2018-01-15', '2018-02-19', '2018-05-28', '2018-07-04', '2018-09-03', '2018-10-08', '2018-11-11', \n",
    "'2018-11-22', '2018-12-25', '2019-01-02', '2019-01-21', '2019-02-18', '2019-05-27', '2019-07-04', \n",
    "'2019-09-02', '2019-10-14', '2019-11-11', '2019-11-28', '2019-12-25') THEN '1' \"\"\" + \n",
    "\"\"\" WHEN FL_DATE in ('2015-01-02', '2015-01-18', '2015-01-20', '2015-02-15', '2015-02-17', \n",
    "                    '2015-05-24', '2015-05-26', '2015-07-03', '2015-07-05', '2015-09-06', '2015-09-08', \n",
    "                    '2015-10-11', '2015-10-13', '2015-11-10', '2015-11-12', '2015-11-25', '2015-11-27', \n",
    "                    '2015-12-24', '2015-12-26',\n",
    "                    '2015-01-31', '2016-01-02', '2016-01-17', '2016-01-19', '2016-02-14', '2016-02-16', \n",
    "                    '2016-05-29', '2016-06-01', '2016-07-03', '2016-07-05', '2016-09-04', '2016-09-06', \n",
    "                    '2016-10-09', '2016-10-11', '2016-11-10', '2016-11-12', '2016-11-23', '2016-11-25',\n",
    "                    '2016-12-24', '2016-12-26',\n",
    "                    '2017-01-01', '2017-01-03', '2017-01-15', '2017-01-17', '2017-02-19', '2017-02-21', \n",
    "                    '2017-05-28', '2017-05-30', '2017-07-03', '2017-07-05', '2017-09-03', '2017-09-05',\n",
    "                    '2017-10-08', '2017-10-10', '2017-11-10', '2017-11-12', '2017-11-22', '2017-11-24',\n",
    "                    '2017-12-24', '2017-12-26',\n",
    "                    '2018-01-01', '2018-01-03', '2018-01-14', '2018-01-16', '2018-02-18', '2018-02-20', \n",
    "                    '2018-05-27', '2018-05-29', '2018-07-03', '2018-07-05', '2018-09-02', '2018-09-04', \n",
    "                    '2018-10-07', '2018-10-09', '2018-11-10', '2018-11-12', '2018-11-21', '2018-11-23',\n",
    "                    '2018-12-24', '2018-12-26',\n",
    "                    '2019-01-01', '2019-01-03', '2019-01-20', '2019-01-22', '2019-02-17', '2019-02-19', \n",
    "                    '2019-05-26', '2019-05-28', '2019-07-03', '2019-07-05', '2019-09-01', '2019-09-03', \n",
    "                    '2019-10-13', '2019-10-15', '2019-11-10', '2019-11-12', '2019-11-27', '2019-11-29', \n",
    "                    '2019-12-24', '2019-12-26') THEN '1' \"\"\" + \n",
    "\"\"\" WHEN DAY_OF_WEEK in ('1', '5') THEN '1' \"\"\"\n",
    "\"ELSE '0' END\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f10255a3-1285-4a28-98e7-bfff26fc3142",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Flight delays can have a snowball effect on other flights. For this reason, we decided to incorporate a `prev_dep_delayed_confirmed` (previous departure delayed confirmed) feature. To achive this task, a window funcion was used based on the tail number and planned arrival time to previous flight information, such as PLANNED_PREV_ARR, PREV_DEP_DELAYED, PREV_DEP_TIME, and PREV_ORIGIN features. Some addtional features and conversions were done to achive this task, for example capturing the schedule departure and arrival time of each flight, and converting local times to UTC time. The actual incorporation of the pev_dep_delayed_confirmed feature was done in Section 4.2 after the AirportStation - Airline datasets were joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d3c3adc-0470-47f8-8f73-4994804e391f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scheduled time in minutes between planned arrival and planned departure\n",
    "df_join_ac_st_airline_4 = df_join_ac_st_airline_3.withColumn('planned_time_between_flights', (unix_timestamp('utc_actual_timestamp')-unix_timestamp('utc_planned_prev_arrival'))/60)\n",
    "\n",
    "# Previous flight delay information if departure was 2 hours ahead of scheduled departure time, if data is not available it will give it a 2 (which would also represent short flights)\n",
    "df_join_ac_st_airline_4 = df_join_ac_st_airline_4.withColumn('prev_dep_delayed_confirmed', when(unix_timestamp('utc_actual_timestamp')-unix_timestamp('utc_prev_departure') > \"7200\" , col('prev_dep_delayed')).otherwise('2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad467685-636c-4ee2-9e7d-5f9c6273ab3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The time between flights `planned_time_between_flights` is positively correlated with flight delays. We decided to make this a feature and use it in our models. This data is already available 2 hours (7200 seconds) before departure and can be used for our flight delay prediction. The previous delay infomation will come from the variable prev_dep_delayed (previous departure delay) as a 0 or 1 label. Flights that not meet that criteria (<7200 seconds) will be labeled as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "28b683c8-adcd-4644-b795-d3c25948f4b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scheduled time in minutes between planned arrival and planned departure\n",
    "df_join_ac_st_airline_4 = df_join_ac_st_airline_3.withColumn('planned_time_between_flights', (unix_timestamp('utc_actual_timestamp')-unix_timestamp('utc_planned_prev_arrival'))/60)\n",
    "\n",
    "# Previous flight delay information if departure was 2 hours ahead of scheduled departure time, if data is not available it will give it a 2 (which would also represent short flights)\n",
    "df_join_ac_st_airline_4 = df_join_ac_st_airline_4.withColumn('prev_dep_delayed_confirmed', when(unix_timestamp('utc_actual_timestamp')-unix_timestamp('utc_prev_departure') > \"7200\" , col('prev_dep_delayed')).otherwise('2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa03e6e7-b8c3-4123-83f1-a4361ec7211a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.7 - EDA post join with full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4ef0bbe-dede-4383-8310-270c80e930ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Once the full datasets were joined, an additional EDA post-join was done for the final dataset. Code can be found [here](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/2297543790314938/command/2297543790314940).\n",
    "\n",
    "Due to scalability issues using Pandas DataFrames, the data visualizations after joining the datasets were done using Databriks Visualizations. Azure Databricks supports various types of visualizations out of the box using the display and displayHTML functions.\n",
    "\n",
    "Using the DataFrame `display` method and `Data Profile` display option were were able to visualize the summary statistics for our full dataset in a tablular and graphic format for categorical and numerical features. The figure below shows a partial screenshot of this visualization.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/EDAnumFeatures.jpg' alt=\"Drawing\" width=\"800\"/>\n",
    "<br>\n",
    "\n",
    "Similarly we ran a test to evaluate the correlation of all our numerical features and the target variable to assist us with feature selection. We know that we can not use the first three features since they are not independent to the target variable.\n",
    "\n",
    "|Feature\t| Correlation with Target|\n",
    "|------------------|----------------|\n",
    "|departure_delay_15|\t0.950224|\n",
    "|arrival_delay_boolean\t|0.698117|\n",
    "|departure_delay\t|0.549628|\n",
    "|prev_dep_delayed_confirmed\t|0.184948|\n",
    "|route_delay\t|0.112306|\n",
    "|wnd_speed_prev_2\t|0.089018|\n",
    "|wnd_speed_prev\t|0.085275|\n",
    "|dest_departure_delay|\t0.074983|\n",
    "|origin_departure_delay\t|0.070791|\n",
    "|dest_arrival_delay|\t0.070083|\n",
    "|origin_arrival_delay|\t0.062202|\n",
    "|wnd_angle_qc_prev|\t0.034732|\n",
    "|wnd_angle_qc_prev_2\t|0.033496|\n",
    "|tmp_c_prev_2|\t0.029291|\n",
    "|wnd_angle_prev|\t0.028105|\n",
    "|wnd_angle_prev_2\t|0.025892|\n",
    "|tmp_c_prev|\t0.023100|\n",
    "|dew_c_prev_2\t|0.020126|\n",
    "|dew_c_prev\t|0.019560|\n",
    "|weather_obs_prev|\t0.018718|\n",
    "|weather_obs_prev_2\t|0.015997|\n",
    "|holiday|\t0.014184|\n",
    "|distance\t|0.012024|\n",
    "|distance_group|\t0.011896|\n",
    "|slp_p_prev_2\t|0.006554|\n",
    "|slp_p_prev\t|0.006038|\n",
    "|planned_time_between_flights|\t-0.000328|\n",
    "|wnd_cloud_angle_prev\t|-0.062745|\n",
    "|wnd_cloud_angle_prev_2|\t-0.065258|\n",
    "|vis_dist_prev_2|\t-0.067110|\n",
    "|vis_dist_prev\t| -0.068335|\n",
    "\n",
    "Once we had a better understanding of the features and results from the correlation dataframes, we decided to visualize some relevant features.\n",
    "\n",
    "**Average Wind Speed for second previous hour by target variable**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda-c104.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "As we can see from the above figure, wind speed has a positive correlation with departure delay.\n",
    "\n",
    "**Percentage of Delayed Flights per Previous Delays by Target Variable**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda-c105.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "This visualization indicates that previous departure delays have a positive correlation with departure delay, with on time previuos departures we have aprox 10% departure delays; and with delays on previous departures there is aprox 40% of departure delays.\n",
    "\n",
    "**Average Route Delay by to target variable**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda-c106.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "As we can observe, we have a positive correlation with route delays and departure delays.\n",
    "\n",
    "**Percentage of Delayed Flights by Holiday**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda-c107.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "There is a one percent difference bewtween non-delayed flights on non holiday days vs delayed flights on non holiday days, thus even thought we expected the holiday feature to impact the delays, we found that holidays don't have an impact in the departure delays.\n",
    "\n",
    "**Departure Delays by Month**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/NB1-delmonth.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "The majority of the delays occur on July, June, August, and December.\n",
    "\n",
    "**Departure Delays by Month and by Year**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda-c111.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "In general, departure delays by month follow similar trends for the years 2015, 2016, 2017, 2018, and 2019. With an increased number of delays on the months of June, July, August, and December.\n",
    "\n",
    "**Percentage of Departure Delays by Carrier**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda-c113.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "The above chart shows how the percetage of the number of delays by carrier varies by year. For example, Frontier (F9) has the most number of delays during 2018 and 2019, but it wasn't in the top 3 during  the period 2015-2017.\n",
    "\n",
    "**Weather Characteristics - Wind Speed**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/eda-c117.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "The above chart shows a positive correlation between weather conditions that can cause delays with departure delays. The conditions considered in this feature correspond with:\n",
    "- Blowing or drifting snow or sand, visibility less than 1 km<br>\n",
    "- Fog<br>\n",
    "- Fog or ice fog in patches<br>\n",
    "- Fog or ice fog, has begun or become thicker during the past hour<br>\n",
    "- Fog, depositing rime<br>\n",
    "- Precipitation, heavy<br>\n",
    "- Solid precipitation, heavy<br>\n",
    "- Freezing precipitation, heavy<br>\n",
    "- Rain, not freezing, heavy<br>\n",
    "- Rain, freezing, heavy<br>\n",
    "- Rain or drizzle and snow, moderate or heavy<br>\n",
    "- Snow, heavy<br>\n",
    "- Ice pellets, heavy<br>\n",
    "- Rain showers or intermittent rain, heavy<br>\n",
    "- Rain showers or intermittent rain, violent<br>\n",
    "- Snow showers or intermittent snow, heavy<br>\n",
    "- Hail<br>\n",
    "- Thunderstorm, heavy, with no precipitation<br>\n",
    "- Thunderstorm, heavy, with rain showers and/or snow<br>\n",
    "- Thunderstorm, heavy, with hail<br>\n",
    "- Tornado<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "534563db-0135-4e52-a7e7-a0f9ff935b31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.8 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5575a166-9695-490b-a935-186b6f6c1d80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The following table contains the variables used for our machine learning algorithms, they include a combination of engineered features, features with highest absolute correlation with the target variable and descriptive features of each flight, route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41ea6975-6989-4573-94ad-f893eb361c16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "|Feature Name | Description | Type |\n",
    "|----------------|------------------|-------------------|\n",
    "|Year | Departure Flight Year | Categorical |\n",
    "|Month | Departure Flight Month | Categorical |\n",
    "|Actual_timestamp | Depature Timestamp in Local Time | Datetime |\n",
    "|Hour | Departure Hour in Local Time | Categorical |\n",
    "|Carrier | Flight Carrier / Airline | Categorical |\n",
    "|Holiday | (Section 2.6) Boolean for flights during holidays, Mondays or Fridays and 1 Day Buffer | Boolean |\n",
    "|Weather_obs_prev | (Section 2.6) Special weather conditions observed during the previous hour | Boolean |\n",
    "|Weather_obs_prev_2 | (Section 2.6) Special weather conditions observed during the second previous hour | Boolean |\n",
    "|Origin | Flight Origin Airport | Categorical |\n",
    "|Destination | Flight Destination Airport | Categorical |\n",
    "|**Departure_delay_boolean** | **Target Variable, Flights delayed more than 15 minutes or cancelled** | **Boolean** |\n",
    "|Planned_time_between_flights | (Section 2.6) Difference between scheduled departure for current flight and scheduled departed from its previous flight | Numerical \n",
    "|Prev_dep_delayed_confirmed | (Section 2.6) Previous flight delayed if latest departure is available at prediction time (-2 Hours) | Categorical |\n",
    "|Distance | Flight distance | Numerical |\n",
    "|Wnd_angle_prev | Wind Angle of Previous Hour | Numerical |\n",
    "|Wnd_type_prev | Wind Type of Previous Hour | Numerical |\n",
    "|Wnd_speed_prev | Wind Speed of Previous Hour | Numerical |\n",
    "|Wnd_speed_prev_2 | Wind Speed of Second Previous Hour | Numerical |\n",
    "|Wnd_cloud_angle_prev| Cloud Height / Ceiling of Previous|Numerical|\n",
    "|Vis_dist_prev| Visibility Distance of Previous Hour|Numerical|\n",
    "|Vis_dist_prev_2| Visibility Distance of Second Previous Hour |Numerical|\n",
    "|Vis_var_prev|Visibility Variance of Previous Hour|Numerical|\n",
    "|Tmp_c_prev|Temperature (Celsius) of Previous Hour|Numerical|\n",
    "|Dew_c_prev|Temperature (Celsius) of Previous Hour|Numerical|\n",
    "|Route_delay|Mean Historical Route Departure Delay|Numerical|\n",
    "|Origin_arrival_delay|Mean Historical Origin Arrival Delay|Numerical|\n",
    "|Origin_departure_delay|Mean Historical Origin Depature Delay|Numerical|\n",
    "|Dest_departure_delay|Mean Historical Destination Depature Delay| Numerical|\n",
    "|Dest_arrival_delay|Mean Historical Destination Arrival Delay| Numerical|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a850e220-b749-4c35-a59a-a2caeffecd6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.9 - Split Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "464f549f-22a0-481c-a190-eaccfefea37e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We divided our data into 2 sets, training and testing. In order to address the time series component of the data we decided to use a cumulative ranking suing a window partition ordered by the departure time `actual_timestamp`. Using this approach we are able to split the data and respect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4949da44-06c4-4df1-b2a3-24c647dfa78b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Code used for data split.\n",
    "split_data_df = split_data_df.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"actual_timestamp\")))\n",
    "train_df = split_data_df.where(\"rank <= .8\").drop(\"rank\",\"actual_timestamp\")\n",
    "test_df = split_data_df.where(\"rank > .8\").drop(\"rank\", \"actual_timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c81840cc-8af1-4916-b9af-5d350e123e45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.10 - Checkpoint Join Results\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/data_pipeline_1.png' alt=\"Drawing\" width=\"800\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f598ca1-103f-46f9-a5ee-697275930ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We joined the Airline, Station, Airport and the Weather datasets and create a checkpoint in Azure storage. We then created a rank for each record to then split the data for train set and test sets. This is then saved in Azure for checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "07b0255e-e260-4bda-abf0-cde41d8173b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85eedf8c-b5f2-482b-9723-46c3caf2d0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1 - Introduction\n",
    "\n",
    "Feature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. This involves testing features, deciding what features to create, creating features, testing the impact of the identified features on the task etc., and is done by numerical transformations (like taking fractions or scaling), category encoder like one-hot or target encoder (for categorical data), clustering, group aggregated values, and Principal Component Analysis (PCA) (for numerical data). The process is typically repeated to improve the features. The objective of feature engineering is to align analysis with the business problem, eliminate unecessary data and promote scalability of the model. \n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/feature-engineering.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "References: <br>\n",
    "https://en.wikipedia.org/wiki/Feature_engineering\n",
    "\n",
    "The feature engineering steps done in this project are detailed in sections below.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/data_pipeline_2.png' alt=\"Drawing\" width=\"1000\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3601181-f8ab-4279-96ee-00c0a0c38777",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the train data after split, we separated the data into Categorical and Numerical. For the Cateorical data we used String Indexer and One-hot encoding. For the Numerical data we did data imputation using Median Imputation followed by Standard Scaling. We then used Vector Assembler to assemble the resultant data and continued with Downsample (for on-time) and Upsample (for delayed) flight data to process them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3abc1729-2bfc-41ce-ab54-0a4eeebb5683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.2 - Handling Imbalanced dataset\n",
    "\n",
    "Machine Learning algorithms tend to produce unsatisfactory results when faced with imbalanced datasets. The algorithms have a bias towards classes which have more instances and they tend to predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class.\n",
    "\n",
    "In this project, we have a highly imbalanced dataset with almost 80% of data belonging to the class that predicts on-time flight information. As per our business case, we need to predict the flight delay which in this case belongs to the minority class with just 20% of data. If we run the algorithm without fixing the imabalance in data, it will lead to a high f1-score for on-time flights as compared to the f1-score for delayed flights, which is not correct. \n",
    "\n",
    "In order to handle the imbalance in the dataset, we went through several options as described below:\n",
    "\n",
    "**1) Oversampling using SMOTE (Pyspark Implementation)**\n",
    "\n",
    "SMOTE Implementation is available [here](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/671119174920056/command/671119174921939)\n",
    "\n",
    "This method creates new synthetic examples in the minority class in order to obtain a balanced dataset matching the majority class. \n",
    "The SMOTE algorithm, stands for 'Synthetic Minority Over-sampling Technique', that addresses this issue of imbalanced classes. It is based on nearest neighbors (determined by the Euclidean distance of data points in the feature space).\n",
    "\n",
    "The feature values of nearest neighbor samples are used to interpolate synthetic feature values to retrieve a certain predefined percentage of additional synthetic samples (over-sampling). Applying this algorithm to the samples of the training datasets that belong to the minority classes finally ends up with a more balanced training set and the removal of the model bias.\n",
    "\n",
    "Oversampling technique uses KNN algorithm to find the nearest neighbor, which is a highly memory intensive algorithm, KNN stores the results of neaest neighbor sample in the memory, hence demanding lot of cluster memory space.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/image.png' width=\"400\" height=\"400\">\n",
    "\n",
    "**2) Undersampling **\n",
    "\n",
    "Undersampling helps balance uneven dataset by keeping all of the data in the minority class and decreasing the size of the majority class((i.e on-time flights), to end up with the same number of records in both majority and minority class. \n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/Undersampling.png' width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "**For the purpose of this project, although undersampling discards data from majority class, we decided to proceed with undersampling technique over SMOTE because of below 2 main reasons:**\n",
    "- Since Cluster memory is limited, the code that has been implemented for oversampling method is giving \"Out of memory\" error. \n",
    "- It is appropriate to use undersampling even after drop in the majority class observation, since we have 4 years of flight data, which is a very large dataset(in millions) and is plenty for an accurate analysis.\n",
    "\n",
    "Undersampling has been done only on the training dataset and test dataset has not been touched at all, in order to maintain a real time prediction scenario.\n",
    "\n",
    "Before implementing undersampling, negative class (i.e on-time flights) had 17930122 rows and positive class (i.e delayed flights) had 4432422 rows in the full training dataset.\n",
    "After downsampling, negative class (i.e on-time flights) has 4389274 rows and positive class has 4432422 rows\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/fe1.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "After downsampling, negative class has 4389274 rows and positive class(i.e delayed flights) has 4432422 rows\n",
    "\n",
    "After handling imbalanced data, the f1 score for Gradient Boosted Tree algorithm increased from 25% to 51%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26f6c260-9ba3-49be-9674-8b779b61436c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.3 - Using PCA for feature reduction\n",
    "\n",
    "PCA is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimension. This dimensionality-reduction method that is used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller set while minimizing information loss and increasing interpretabililty, such that most of the information in the data is preserved. Reducing the number of features improves the efficiency of machine learning algorithms, facilitates data analysis tasks, but it is also helpful for data visualization. PCA was used during this project for data reduction, code can be found in this [Exploration (PCA) Notebook URL](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/188115645374385/command/188115645375889). A total of 811 features were reduced to 120 principal components to explain 96% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7ca0051-2a72-4f69-b571-8e3d672939e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.4 - Numerical Data Imputation\n",
    "\n",
    "Median for missing values, median doesn't affect the computation\n",
    "\n",
    "Missing values occur when there is no value is stored for the variable in the column and is a common problem that can have a significant effect on the conclusions due to the reduction in statistical power. Bias is caused in the estimation of parameters due to missing values and the importance of the samples are reduced.\n",
    "\n",
    "Data Imputation is the process of replacing the missing data with approximate values. Instead of deleting any columns or rows that has any missing value, this approach preserves all cases by replacing the missing data with the value estimated by other available information.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/imputation.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "Structured missing data can not be treated as they are not meant to contain any information. These are simply imputed with 0 value if numeric or some different category if object. Imputation of missing data is done under the assumption that the data is Missing at Random (MAR).\n",
    "\n",
    "The different approaches for addressing data imputation are: Mean/Median Imputation, Mode substitution, Regression, Maximum Likelihood, Stochastic Regression Imputation, Hot-Deck Imputation, Cold-Deck Imputation \n",
    "\n",
    "We used the Mean/Median Imputation, where we used the median value of a variable in place of the missing data value for that same variable. This is simple to understand and apply but can lead to bias in multivariate variables such as correlation or regression coefficients and may not be very accurate. This also doesn’t account for the uncertainties in the imputations. However, when considering the differences between the mean and the median. The median will be less affected by outliers, which alllows the preservation of the original distribution of the data. Therefore, we have decided to use the Median for the imputation of numerical features.\n",
    "\n",
    "\n",
    "\n",
    "Reference: <br>\n",
    "https://medium.com/analytics-vidhya/ways-to-impute-missing-values-in-the-data-fc38e7d7e2c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2f6269f0-8e3c-4ef1-9e68-45222e1c837c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 4 -  Algorithm Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cc57354a-d938-4cb3-a0f1-0d9822b4a5dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.1 - Introduction\n",
    "\n",
    "[Link to Algorithm Exploration notebook](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/1424087319433391/command/1424087319433467)\n",
    "\n",
    "In addition to the baseline model (Logistic Regression (LR)), we also trained and evaluated below models to explore and finalize on the final model to be used as an outcome of our research. Pipeline setup was also done to run different stages of data prep before the actual model run and evaluation.\n",
    "\n",
    "- Random Forest (RF)\n",
    "- Gradient Boosted Tree (GBT)\n",
    "- XGBoost (XGB) (**selected as a final algorithm where we will spent further efforts on fine tuning**)\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/data_pipeline-3.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c26453f9-be16-4aee-9236-4bcbbf24e1e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.2 - Model 1 (Baseline): Logistic Regression\n",
    "\n",
    "We choose Logistic Regression as our baseline model as it is quite effective in determining a baseline relationship. It is simple, efficient, easy to implement and scales well, in particular for classification. This will help us identify important features and relationships between features. We will have to scale the features before applying the model. Logistic Regression uses the Sigmoid function which returns a probability between 0 and 1. This transforms continuous infinite scale into a scale between 0 and 1 (Sigmoid Function or Sigmoid activation). We have the flexibility to decide on the decision \"threshold\". We have chosen to use 0.5 as the threshold. Probability greater than 0.5, means prediction class is 1 (flight departure delayed) and probability less than 0.5 means prediction class is 0 (flight not delayed). We did do some fine tuning of our thresholds in later runs too - it is important to be mindful of the threshold probability in a type of problem like \"airline delay classification\" - because the threshold decision dictates how much confidence we want to put in the threshold when deciding a delay - a decision that impacts both customer experience and airline operating expenses.\n",
    "\n",
    "Logistic Regression assumes linearity between the dependent and independent variables which is one of its limitations which makes makes it difficult to model complex real-world relationships. Logistic regression requires low or no multicollinearity between variables. If the dataset has high dimensions, this can lead to overfitting. As a result feature selection should avoid features that are highly correlated. In our implementation, we choose one of the correleated features. \n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/LogReg.png'>\n",
    "<br>\n",
    "\n",
    "\n",
    "In our project, Logistic regession model has been trained on the training dataset to find an ordinary least squares regression line of Y(departure_delay_boolean) and X(set of variable selected as ML features, all of which have been transformed into a vector using VectorAssembler)\n",
    "- Grid search was used to find the best hyperparameter values for parameters such as threshold, regParam, fitIntercept, elasticNetParam and maxIter. \n",
    "- The threshold value used for our model is 0.6, which means that if the output probability of the Logistic regression model is  greater than 0.6, the flight delayed is predicted to happen otherwise flight will be on time.\n",
    "\n",
    "**The f1 score that was achieved using logistic regression model is 48%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72d37ef8-d9ca-4bf9-b5d2-1d4f1c3802a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Model 1: Logistic Regression Results**\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/lr-1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/lr-2.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/lr-3.png' width=\"400\" height=\"400\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42913301-d5af-49be-9dc4-27362ea2b583",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.3 - Model 2: Random Forest Classifier\n",
    "\n",
    "We have choosen Random Forest Classifier as our second model as it addresses the problem of overfitting quite well compared to decision trees for complex hypothesis tree. As a result, it makes sense to use of ensemble learning to gain collective classification knowledge for the flight departure time prediction problem. Random Forest which is an ensemble of decision trees, uses bagging technique where each decision tree is fit to a sample taken from the entire dataset. Its success depends on using uncorrelated decision trees which is done using bootstrapping. The trees are grown independently in the model and the result is determined by taking a majority vote from all results. Random Forest can handle higher dimensionality and also include features with higher correlation. Random Forest can identify the most significant features via \"feature importance\" scoring, features do not have to be encoded or scaled and it can handle null and missing values. \n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/random-forest.jpg' width=\"700\" height=\"900\">\n",
    "<br>\n",
    "\n",
    "In this project, for Random forest classifier implementation, The input columns that have been used are `labelCol` and `featuresCol` and the parameter been used is maxDepth: Maximum depth of each tree in the forest.\n",
    "- Increasing the depth makes the model more expressive and powerful. However, deep trees take longer to train and are also more prone to overfitting. Hence we used maxDepth as 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54868053-aaa7-4dca-874f-356c85367daa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Model 2: Random Forest Classifier Results**\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/rf-1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/rf-2.png' width=\"300\" height=\"300\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/rf-3.png' width=\"400\" height=\"400\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a46a1ebe-031f-49f3-8754-f4916ce3d324",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.4 - Model 3: Gradient Boosted Trees\n",
    "\n",
    "The third model we explored is Gradient Boosted Trees which is a set of decision trees. It uses boosting technique as opposed to bagging in Random Forest. Each tree is grown sequentially by using information from the previous tree in order to minimize residual errors. Gradient Boosted Trees are not fit to the entire dataset which helps correct our mistakes better compared to Random Forest but at the cost of needing longer to train (the next set of trees can train after mistakes from the previous set of trees are understood) with large datasets with some challenges with tuning the model. Boosting emphasizes small errors and noise which makes the number of trees an important consideration and too many trees may cause overfitting unlike Random Forest. Gradient Boosted Trees do not require bootstrapping. Also, each decision tree is fit to the residuals from the previous one and correlated trees isn't an issue.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/gbt.png' width=\"500\" height=\"700\">\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "For the Implementation of Gradient Boosted Trees algorithm, best parameters were obtained from the results of cross validation to train the model. Due to the limited processing availability (shared cluster), we took a limited approach for cross validation by taking a small sample(0.1%) of the full training dataset to find the best parameters. Based on the results of cross validation on 3 folds, we opted for maxDepth=10, maxBins=10, and maxIter=10, which is a combination of hyperparameters that achieved the highest average f1 score during the cross-validation process. These params have been used for running GBT algorithm. The full results of cross-validation can be found under MLlib experimentation section of \"Final_Project_nb3_team_11_Algorithm_Exploration\" notebook. \n",
    "\n",
    "- The input columns that have been used are `labelCol` and `featuresCol`\n",
    "- The parameters used are: maxBins, maxDepth, minInstancesPerNode, minInfoGain, stepSize and maxIter\n",
    "\n",
    "**Cross-validation:**\n",
    "\n",
    "We used f1 as our evaluation metric in the cross-validation process, as this was the same metric that has been used in training and evaluating our models. We used k=3 folds to split the dataset with training and test partition. CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. with k=3 folds, CrossValidator will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs.\n",
    "After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d44691a2-0804-47ae-baa0-dcb4db326fc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Model 3: Gradient Boosted Trees Results**\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/gb-1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/gb-2.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/gb-3.png' width=\"400\" height=\"400\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ba1f1e3-90ea-45d6-be51-57c16862383b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.5 - Model 4: Support Vector Machine\n",
    "\n",
    "We chose Support Vector Machine (SVM) as the fourth model to explore. SVM is a linear model for classification and regression problems that can solve both linear and non-linear problems. The algorithm creates a line or a hyperplane with a margin that maximizes a linearly separable hyperplane to separate data into 2 classes. \n",
    "\n",
    "The objective of this algorithm is to find a hyperplane in an N-dimensional space (N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "We used the LinearSVC class in PySpark because we are doing support vector classification - and not regression. It is important to note that LinearSVC in Pyspark optimizes the Hinge Loss using the OWLQN optimizer. It only supports L2 regularization currently.\n",
    "\n",
    "SVM requires more computation time for achieving almost the same accuracy as with other classifiers such as Random forest.\n",
    "SVM is well-suited for two-class problems. It maximizes the \"margin\" and thus relies on the concept of \"distance\" between different points. One-hot encoding for categorical features is needed with SVM. Further, min-max or other scaling is highly recommended at preprocessing step for SVM.\n",
    "For a classification problem like the one that we built our model on, Random Forest gives the probability of belonging to class whereas SVM gives the distance to the boundary, Hence, for those problems, SVM generally performs better than Random Forest. But the caveat here is that, for SVM, we still need to convert it to probability somehow if we need probability.\n",
    "\n",
    "SVM tries to find an optimal hyperplane rather than focusing on maximizing the probability of the data and is less prone to overfitting compared to logistic regression.\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/svm.png' width=\"500\" height=\"700\">\n",
    "<br>\n",
    "\n",
    "\n",
    "For the purpose of implementing Support Vector machine in our research, we have used LinearSVC which in Spark ML supports binary classification with linear SVM.\n",
    "- Grid search was used to find the best hyperparameter values for parameters such as threshold, regParam, fitIntercept and elasticNetParam, maxIter and aggregationDepth. \n",
    "- Used MulticlassClassification Evaluator for evaluating the model using metricName as 'f1'\n",
    "- Used input columns `featuresCol` and `labelCol` for creating the linearsvc model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ebd9edce-b043-4793-a677-c096626c3e42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Model 4: Support Vector Machine Results**\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/svm-1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/svm-2.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/svm-3.png' width=\"400\" height=\"400\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32485a07-79c1-4dd3-a5b6-0d4dc6748788",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.6 - Model 5: XGBoost\n",
    "\n",
    "XGBoost is a fast and parallalizable ensemble learning algorithm that combines the results of base learners (Decision Trees) to make a prediction. The trees in this classifier contain real-value scores of whether an instance belongs to a group. A threshold is used to make a decision after reaching the max depth of the tree by converting the scores to categories. We train a sequence of models, with each emphasizing the examples misclassified by the previous model. The objective is to combine a set of classifiers to reduce overfitting by maintaining a weight of each training example. A uniform distribution is assumed for the training examples inorder to train the classifier. The classifier is run on the training examples, to determine which examples are correctly classified and the weights for these exmples are reduced and vice versa. The process is then repeated to train subsequent classifiers until the stopping criteri is met. \n",
    "\n",
    "XGBoost hyperparameter tuning was attempted to find the number of decision trees and max depth of each decision tree. We haven't been able to complete the execution before the deadline for project submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49823eb8-5912-44d0-9349-0932b23fa6cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Model 5: XGBoost Results**\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/xgb-1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/xgb-2.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/xgb-3.png' width=\"400\" height=\"400\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a9ef905-2be0-4452-ae6b-2e2b24cad64b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.7 - Algorithm Exploration - Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ffa133f-3cd9-43e5-8d44-4ce879e02b7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### Model Performance: Metrics Comparison\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/model-results1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/model-results2.png' width=\"1000\" height=\"1000\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/model-results3.png' width=\"400\" height=\"400\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6630c0cb-35f7-4982-9dcd-86ff6053c4ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.8 - Algorithm Exploration with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "acfdb0e6-0633-4af0-b0a0-273204fc2afb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As mentioned before, Principal Component Analysis is a dimensionality-reduction method that is used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller set while minimizing information loss and increasing interpretabililty, such that most of the information in the data is preserved. Reducing the number of variables of a dataset can negatively affect the accuracy, but the goal in dimensionality reduction is to trade a little accuracy for simplicity. Reducing the number of features of a dataset, while preserving as much information as possible can have great benefits, for example, datasets are easier to explore and visualize facilitating data analysis tasks, making the tasks easier and faster for machine learning algorithms to process.\n",
    "\n",
    "Standarization prior PCA is a critical step in Machine Learning. The goal is to standarize the range of continuous variables so that each one will contribute equally to the analysis, so that larger ranges won't dominate over features with small ranges. Once the standarization is done, the variables will be transformed to the same scale. StandardScaler was used to standarize the dataset features for all our models.\n",
    "\n",
    "In addition to Standarization, Explained Variance Ratio \"explainedVariance\" was used to better understand how much information (variance) can be attributed to each of the principal components. A total of 120 principal components were used to explain 96% of the variance.\n",
    "\n",
    "PCA was added to the ML pipeline, and implemented for Logistic Regression, Random Forest Classifier, Gradient Boosted Trees, and Support Vector Machine algorithms (Models 1 - 4) mentioned above.  Results can be found in the following sections (4.6.1 - 4.6.4).\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/data_pipeline-4.png' alt=\"Drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3640c771-a198-4daf-b7e9-53331faa1257",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.9 - Algorithm Exploration with PCA - Results\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/pca-1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/pca-2.png' width=\"600\" height=\"600\">\n",
    "<br>\n",
    "\n",
    "\n",
    "**Logistic Regresion**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/pca-lr-1.png' width=\"600\" height=\"600\">\n",
    "<br>\n",
    "\n",
    "\n",
    "**Random Forest**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/pca-rf-1.png' width=\"600\" height=\"600\">\n",
    "<br>\n",
    "\n",
    "\n",
    "**Gradient Boosting Trees**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/pca-gb-1.png' width=\"600\" height=\"600\">\n",
    "<br>\n",
    "\n",
    "\n",
    "**Support Vector Machine**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/pca-svm-1.png' width=\"600\" height=\"600\">\n",
    "<br>\n",
    "\n",
    "\n",
    "**XGBoost**\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/pca-xgb-1.png' width=\"600\" height=\"600\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1d78d98f-a0a5-43b0-8365-68dda2162553",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.10 - Model Performance with PCA: Metrics Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "406b8ca6-f61c-4da7-9ea0-cf601a89baa6",
     "showTitle": false,
     "title": "Model Performance: Metrics Comparison"
    }
   },
   "source": [
    "###### Model Performance: Metrics Comparison\n",
    "\n",
    "| Metric          | LR | RF     |  GBT          | SVM | XGB |\n",
    "|-----------|---------------------|-------------------|-------------------|------------------------|-------|\n",
    "| F1 Score (0) | 0.89   | 0.82 | 0.83 | 0.81      |  0.85  |\n",
    "| F1 Score (1) | 0.47   | 0.46 | 0.49 | 0.46      |  0.53  |\n",
    "| Precision | 57.30   | 39.44 | 40.44 | 37.40        |  0.45  |\n",
    "| Recall    | 39.17   | 57.80 | 62.11 | 58.24       |  0.64 |\n",
    "| Accuracy  | 0.82   | 0.73 | 0.74 | 0.72      | 0.77 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf7a3b3c-9a6c-488d-8eac-0d214725e794",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.11 - Time Series Split Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5cd2014d-ed8d-43ec-b300-9014d7c20add",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Note: This can be implemented in a future iteration of this project, but was not implemented in this iteration.*\n",
    "\n",
    "We have a time series dataset and there is a temporal meaning in the ordering of the rows in the data and that must be preserved during training and testing. Thus, to accommodate for this, we choose to train the data in \"window iterations\". We wanted to explore this approach to see any diference in the evaluation metric.\n",
    "\n",
    "We have written a custom function to do this for us. This can be found in [this](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/2297543790307072/command/2297543790320734) notebook: . We choose to implement this in chunks of 1 year. Below is how the whole datset has been split:\n",
    "\n",
    "**For the first iteration, we select the following dates for train and test:**\n",
    "\n",
    "Train - (datetime.datetime(2015, 1, 1, 0, 0), datetime.datetime(2015, 12, 31, 0, 0))\n",
    "Test - (datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 12, 31, 0, 0))\n",
    "\n",
    "**For the second iteration, we select the following dates for train and test:**\n",
    "\n",
    "Train - (datetime.datetime(2015, 1, 1, 0, 0), datetime.datetime(2016, 12, 31, 0, 0))\n",
    "Test - (datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 12,31, 0, 0))\n",
    "\n",
    "**For the third iteration, we select the following dates for train and test:**\n",
    "\n",
    "Train - (datetime.datetime(2015, 1, 1, 0, 0), datetime.datetime(2017, 12, 31, 0, 0))\n",
    "Test - (datetime.datetime(2018, 1, 1, 0, 0), datetime.datetime(2018, 12, 31, 0, 0))\n",
    "\n",
    "**For the fourth iteration, we select the following dates for train and test:**\n",
    "\n",
    "Train - (datetime.datetime(2015, 1, 1, 0, 0), datetime.datetime(2018, 12, 31, 0, 0))\n",
    "Test - (datetime.datetime(2019, 1, 1, 0, 0), datetime.datetime(2019, 12,31, 0, 0))\n",
    "\n",
    "We continued this pattern till we have exhausted all 5 years of data. \n",
    "\n",
    "We can think of the above as an expanding window strategy, where the training data starts from the beginning of the dataset in each iteration of the model and expands by 1 year in every subsequent iteration. The test data begins where the training data date range ends and the test data ends at a fixed interval (1 year) from its own starting point. (Visual below)\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/cv-1.png' alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1386b835-5c78-49c4-abb4-6919d603ffb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 5 -  Algorithm Implementation - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1fdbefb1-eee6-4c88-8731-1e638a03b636",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.1 - Overview\n",
    "\n",
    "We have used Logistic Regression as our baseline model and have chosen this model as it is commonly used, simple to implement and the results are easy to interpret. Logistic regression is a classification algorithm that uses continuous and/or categorical predictor variables to classify an outcome variable into one of two categories. In this project the two categories are flight departure delayed or not delayed. Logistic regression can be applied to problem with more than two outcome categories. This is done by creating a separate equation for each category: A/not A, B/not B, C/not C... The outcome variable is a probability (ranging 0 to 1) of group membership. The classification with the highest probability is the predicted class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b23e67d-8c65-4b91-b143-a51e614decc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.2 - Equation\n",
    "\n",
    "Logistic regression aggregates the predictor variables similar to Linear Regression. The input \\\\(X_j\\\\) is multiplied by a weight \\\\(beta_j\\\\) and the product \\\\(X_j \\beta_j\\\\) is added as shown below:  \n",
    "\n",
    "$$\\displaystyle f(X)= \\beta_0 + \\Sigma_{j=1}^p X_j \\beta_j$$\n",
    "\n",
    "This can be expressed as \\\\(f(X)= \\theta^TX\\\\) in matrix form, where \\\\(\\theta\\\\) is a vector of weights including beta_0 \\\\( \\beta_0 \\\\), and \\\\(X\\\\) is a vector of inputs (with an input of \\\\(0\\\\) for \\\\(\\beta_0\\\\). Logistic regression embeds the output of \\\\(\\theta^TX\\\\) in a new funtion \\\\(g(z)\\\\) where $$\\displaystyle g(z)=\\frac{1}{1+e^{-z}}$$ \n",
    "\n",
    "This can be expressed as: $$h_\\theta (x) = g(\\theta^Tx)$$ where \\\\(g(z)=\\frac{1}{1+e^{-z}}\\\\) \\\\(g(z)\\\\) is a sigmoid function, and it scales all outputs values to between 0 and 1. By substituting \\\\(\\theta^TX\\\\) for \\\\(z\\\\), the simplified equation is as follows: \n",
    "\n",
    "$$\\displaystyle h_\\theta (x) = \\frac{1}{1+e^{-\\theta^TX}}$$ \n",
    "\n",
    "The value $$h_\\theta(x)$$ is the probability estimate that \\\\(x\\\\) is a member of category \\\\(y=1\\\\) The probability that \\\\(y=0\\\\) will then be $$1 - h_\\theta(x)$$ \\\\(h_\\theta(x)\\\\) ranges from 0 to 1 due to the application of the sigmoid function and both probabilities will add to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "476efe4d-d8a1-4367-8ed2-3b0725ac0193",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.3 - Cost Function\n",
    "\n",
    "The cost or loss function computes the error of the model. The weights used in logistic regression equation can vary from one model to another. The goal of a model is to fit the data that minimizes the cost function. Comparison of model performance can be done by calculating the error of the models when attempting to predict label \\\\(y\\\\).   \n",
    "\n",
    "For logistic regression, the squared loss function is not convex and has many local minima and alternatives like hinge loss and logistic loss function is used. \n",
    "For logistic loss, the negative log of the logistic regression output is taken when the actual value of \\\\(y\\\\) is 1. When the actual value of \\\\(y\\\\) is 0, the negative log of 1 minus the logistic regression output is used. \n",
    "\n",
    "This can be expressed as:\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/cf1.png' width=\"400\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "When the logistic regression predicts \\\\(\\hat{y}=1\\\\) with a probability of 1 correctly, then \\\\(-log(1)=0\\\\) and the loss function is zero, this is a perfect prediction. Similarly, when \\\\(\\hat{y}:0\\\\) is correctly predicted with a probability of 1, the cost function will be \\\\(-log(1-1)=0\\\\). For an incorrect prediction of \\\\(P(\\hat{y}:0)=.999\\\\), (and the corresponding probability \\\\(P(\\hat{y}:1)=.001)\\\\) but \\\\(y=1\\\\), then the log loss function will be \\\\(-log(.001)\\approx3\\\\) showing a higher amount of error. Since we can't take the log of 0, values of .999 and .001 are used. As the correct prediction approaches a probability of 0, the log loss function will approach infinity and the prediction is \\\\(y=0\\\\)\n",
    "\n",
    "Reference: <br>\n",
    "https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "88de0aac-c5ec-469b-8fc7-00dccb01bcfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.4 - Modeling\n",
    "\n",
    "The weights in logistic regression can be selected at random, and the cost function can be evaluated to see if the new model is an improvement over the last but this is inefficient. The cost function has a slope of zero at its minimum and taking a derivative of the cost function to obtain the slope, and then moving to the next iteration closer to zero, we can find a minimum of the cost function. However, we have to make sure that we are moving in the right direction to find a minimum, since the derivative of the maximum of the cost function will also have a slope of zero. Several different algorithms including Gradient Descent, Newton methods, and quasi-Newton methods can be used that apply some variation of this approach. \n",
    "\n",
    "In Gradient Descent, the first-order derivative of the cost function is evaluated which provides the slope or gradient. The next step is taken based on the greatest negative change in gradient. The learning rate or the step-size is constant for each step and is set by the user. With multiple iterations, the minimum is reached. We will use this method in our toy logistic regression implementation. \n",
    "\n",
    "Newton's method also uses the second-order derivative information to provide the function for a line that is tangent to the slope. This can be traced back to the x-intercept to find a new estimate for the next iteration of where the slope is zero. This method tends to converge in less iterations to the minimum value of the cost function compared to Gradient Descent. However, this method can be computationally expensive and for a non-convex function, Newton's method may converge on a local minima rather than a global minima. \n",
    "\n",
    "Quasi-Newton methods is less computationally expensive but still returns reasonable approximations using a variety of techniques like the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. The aim is to approximate the second-order derivative instead of calculating it outright using a hill-climbing technique that (similar to gradient descent) that iterates through successive approximations of the matrix that are progressively more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb2b2d16-3ece-452a-823a-18eaa5dc80ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.5 - Toy Example Implementation using Logistic Regression (Gradient Descent)\n",
    "\n",
    "As part of this project, we created a toy example implementation using Logistic Regression with Gradient Descent applied on RDDs using 0.01% of the full dataset which includes all the features. This implementation can be found in the following link: [Link to Toy Example Implementation notebook](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/188115645375204/command/188115645375244)\n",
    "\n",
    "We included Ridge Regression to increase the generalizability of the model by adding a regularization or penalty factor. Thus,  taking advantage of the bias-variance tradeoff by shrinking the model coefficients towards 0 which reduces the variance of our model with little increase in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72d98af8-b5ee-4696-af3d-80285ec80181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As it is discussed above Logistic regression uses the sigmoid function to solve classification problems.\n",
    "\n",
    "Thus using the sigmoid function:\n",
    "  \n",
    "$$h_\\theta (x) = \\frac{1}{1+e^{-\\theta^TX}}$$\n",
    "\n",
    "Where the cost function is given by:\n",
    "\n",
    "$$ cost(h_{\\theta}(x),y)=-y^i \\times \\log(h_\\theta (x^{i})) - (1-y^i) \\times \\log(h_\\theta (x^i))$$\n",
    "\n",
    "Therefore, the loss function for logistic regression, when dealing with a vector of n parameters, is defined as it follows: \n",
    "\n",
    "$$ J(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\left(x^i\\times\\log(h_\\theta (x^i))+(1-y^i)\\times\\log(h_\\theta (x^i))\\right)$$\n",
    "\n",
    "In gradient descent we aim to find the minimum of a  differentiable function trying different values an updating them to reach the optimal levels. Thus, minimizing the differentiable function. \n",
    "\n",
    "$$\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$\n",
    "\n",
    "In order to minimize the function we need to run the gradient descent on each parameter of the weight vector (W).\n",
    "\n",
    "Assume we have a total of n features. In this case, we have n parameters for the weight vector vector. To minimize our cost function, we need to run the gradient descent on each parameter of the W  vector.\n",
    "\n",
    "In order to use gradient descent we need to calculate the derivative of the function:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}\\left((h_\\theta)x^i-y^i \\right)x^i_j$$\n",
    "\n",
    "It is important to point out that we are using ridge (L2) regularization to increase the generalizability of our model. Thefore we need to add the term for the penalty (without including the Bias term) which is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}\\left((h_\\theta)x^i-y^i x^i_j + \\lambda x \\right)$$\n",
    "\n",
    "\n",
    "And then updating using the learning rate parameter in the previous equation, which provides the new model for this iteration.\n",
    "\n",
    "$$\\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}\\left((h_\\theta)x^i-y^i x^i_j + \\lambda x \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6872b757-808d-48e7-8847-f54dcc0396c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Toy Example Implementation Results**\n",
    "\n",
    "\n",
    "| Metric          | Logistic Regression |\n",
    "|-----------|---------------------|\n",
    "| F1 Score (1) | 0.04  | \n",
    "| F1 Score (0) | 0.88  |\n",
    "| Precision | 0.33   | \n",
    "| Recall    | 0.02   | \n",
    "| Accuracy  | 0.78   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf83a6e8-efdb-4c9a-baa1-92fcf665abbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5.6 Algorithm Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac7cbf39-469e-4db9-85a7-83e1be11e19c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. GridSearch does an exhaustive search that is performed on a the specific parameter values of a model. We will be using Grid search since it can save us time, effort and resources. Experiments results saved in MLFlow were used to modify the algorithms in the [Implementation Notebook](https://adb-6759024569771990.10.azuredatabricks.net/?o=6759024569771990#notebook/2297543790306469/command/2297543790344697). We tried various metrics, Weighted F1 Score, FMeasure(1), AUC PR curve in order to increase the efficiency of our model. Below is an example of the code use to run the experiments for Logistic Regression.\n",
    "\n",
    "We also ran a gridsearch experimente for Gradient Boosted Trees our best model. However, we decided to only use the metric that yielded the best results for Logistic Regression as it is much more computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "181535c1-c4c5-4947-8f8a-30f39bed6e4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = F1\n",
    "# Code block used for hypertuning including 3 different metrics\n",
    "lr = LogisticRegression(featuresCol='VectorAssembler_features',labelCol='departure_delay_boolean')\n",
    "\n",
    "paramGrid_lr = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.1, 0.001])\n",
    "               .addGrid(lr.fitIntercept, [False, True])\n",
    "               .addGrid(lr.elasticNetParam, [0, 0.5, 1])\n",
    "               .addGrid(lr.threshold, [0.45, 0.5, 0.65])\n",
    "               .addGrid(lr.maxIter, [10, 50])\n",
    "               .build())\n",
    "  \n",
    "# Unweighted F1 Score\n",
    "  if metric == 'F1':\n",
    "  lr_evaluator = MulticlassClassificationEvaluator(labelCol='departure_delay_boolean', metricName='fMeasureByLabel', metricLabel=1, beta=1)\n",
    "  \n",
    "  if metric == 'F1_1':\n",
    "#  Weighted F1 Score  \n",
    "  lr_evaluator = MulticlassClassificationEvaluator(labelCol='departure_delay_boolean', metricName='f1')\n",
    "\n",
    "  if metric == 'PR':\n",
    "# Area under PR Curve\n",
    "  lr_evaluator = BinaryClassificationEvaluator(labelCol=\"departure_delay_boolean\", metricName=\"areaUnderPR\")\n",
    "\n",
    "lr_cv = CrossValidator(estimator = lr,\n",
    "                        estimatorParamMaps = paramGrid_lr,\n",
    "                        evaluator = lr_evaluator,\n",
    "                        numFolds = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "487cb2d8-ff50-46c4-9f49-e8c5a917ea32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5.6.1 Logistic Regression  with Weighted F1 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b23ae7a-b074-45c5-9282-f029eb622b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below are the best parameters that we obtained from our experiments in order to hypertune our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "19942e69-410f-4077-9a1a-779d034d4aa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "|Name\t| Weighted F1 | fMeasure(1) | AUC PR|\n",
    "|-----------|---------------------|\n",
    "|elasticNetParam |\t0.0 |\t0.0 | 0.0|\n",
    "|fitIntercept |\tFalse |\tFalse | False |\n",
    "|maxIter |\t50 |\t50 |  50 |\n",
    "|mlModelClass |\tLogisticRegression |\tLogisticRegression |\tLogisticRegression |\n",
    "|regParam |\t0.001 |\t0.1 | 0.001 |\n",
    "|threshold |\t0.5 |\t0.45 | 0.45 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7f6285a-1f37-454e-9cae-2df8df63faab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "| Metric          | Weighted F1 | fMeasure(1) | AUC PR |\n",
    "|-----------|---------------------|--------------|\n",
    "| F1 Score (1) | 0.48  | 0.46  | 0.46 | \n",
    "| False Positive Rate | 0.27  | 0.37  | 0.35 |\n",
    "| False Negative Rate | 0.34  |  0.26  | 0.27 | \n",
    "| Precision | 0.38   |  0.33   | 0.34 |\n",
    "| Recall    | 0.66   |  0.73   | 0.73 |\n",
    "| Accuracy  | 0.38   |  0.65   | 0.66 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0206dc87-1f8d-4995-9f6a-e87e318b09a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### 5.6.2 Gradient Boosted Tree with Weighted F1 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14321d67-e4c7-43cc-874d-f2d16dfaf016",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "|Name | fMeasure(1) |\n",
    "|-----------|-------|\n",
    "|maxBins |\t10 |\n",
    "|maxDepth |\t10 |\n",
    "|minInstancesPerNode |\t10 |\n",
    "|minInfoGain |0.001 |\n",
    "|stepSize |\t0.2 |\t\n",
    "|maxIter |10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "931b0aa7-dc97-46ef-8a50-0002115af140",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.8 Model Execution Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "90db610a-4e36-4257-9ce3-e57b21b357bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Based on the requirements for the implementation we wanted our stakeholders to be able to evaluate and decide whether they would prefer a higher metric or a lighter algorithm that could be trained faster. Thus, allowing the model to be constantly updated. Therefore, we decided to provide data on the execution time that occured after receiving a balanced trained and a test set. We also tested the impact of adding checkpoints to our pipelines to see if that could decrease the execution times.\n",
    "\n",
    "Here is a diagram that explains the different paths that we evaluated during our implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c96d14c-1957-44d2-a443-e7ff0e1a77bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below are the model execution times from Algorithms Implementation with hyperparameter tuned parameters (where possible).\n",
    "\n",
    "|Model|\tCheckpoints|\tTime (s)|\n",
    "|--------|------------|------|\n",
    "|LR|\tNo Checkpoints|\t1662.8|\n",
    "|LR|\t2 Checkpoints|\t849.8|\n",
    "|LR|\t1 Checkpoint|\t2588|\n",
    "|GBT|\tNo Checkpoints|\t4256|\n",
    "|GBT|\t2 Checkpoints|\t3125|\n",
    "|GBT|\t1 Checkpoint|\t6848|\n",
    "|XGB|\t1 Checkpoint|\t1551*|\n",
    "|XGB|\t2 Checkpoint|\t992*|\n",
    "\n",
    "* The algorithm run with a different cluster size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23342128-ed9b-4bc2-b2e3-11b5110a1e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see on the table above 2 checkpoints required a shorter time to be trained and print results compared to other models where we did not use 2. This jobs were run when the cluster had 11 nodes available so all of them, with the exception of XGBoost ran with the same resources available. We can also observe that just having one internal checkpoint had a negative impact in the peformance of our models. However, having the checkpoint with the transformed data and the internal checkpoint combined yielded the best results.\n",
    "\n",
    "Comparing between algorithms we saw differences in the execution times when comparing Logistic Regression to Gradient Boosted Trees, which ran on the same resources. We were surprised to see that our XGBoost algorithm performed better than Gradient Boosted trees, particularly in execution. However, but we cannot draw a direct comparison between both as they used different cluster sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ae7085a-cf98-41d6-bbf8-3a38c6e4d22a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.9 Final Results from Implementation of XGBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6970fe40-414d-4aa2-992f-22984fadf908",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/final-model-results-1.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/final-model-results-3.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/final-model-results-2.png' alt=\"Drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "Our highest F1-score was from XGBoost Algorithm implementation. We were able to get a F1-score of 53% which is lower than objective. However, we are very confident that with further feature extraction, upsampling of delayed flights data, PCA and hyperparameter tuning, we will be able to achieve our reasearch goal of achieving a F1-score of 60% or higher for predicting flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9a20730-6cec-4bf5-9d62-4a14c3c108c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 6 -  Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe530794-9972-440b-abb1-4ebeb098a1ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this project, we used classification techniques to classify flight delays with high confidence using F1-score as a metric for evaluation. The motivation behind choosing this metric was to keep the needs of both the passengers and the airline as the stakeholders of the research. \n",
    "\n",
    "The weather data did provide some correlation and signal but not enough to get reasonable metrics. The aircraft delays and in between flight time appeared to be relevant and was used to improve the F1-score.\n",
    "\n",
    "Overall compared to the state-of-the-art metrics reported in the literature our models are not bad in terms of accuracy. We are getting 80%-82% accuracy while the state-of-the-art accuracy ranges from 85% to 94% depending on algorithms and threshold. The highest state of the art of 94% accuracy is achieved for 1 hour ahead prediction, while our problem is designed for 2 hours ahead prediction. However, our highest F1-score which is the most important metric here is 53% for XGBoost (without hyperparameter tuning) which is lower than our hypothesis and Reasearch Question. However, we are very confident that with further feature extraction, upsampling of delayed flights data, PCA and hyperparameter tuning, we will achieve our research goal (F1-score of 60%) and fail to reject our hypothesis. \n",
    "\n",
    "Based on the calculated metrics, **XGBoost outperforms all other implemented models in terms of F1-score and overall performance and we selected XGBoost as our final model**. However, XGB and GBT are very competitive. In terms of simulation time, both GBT and XGBoost are much faster than RF (at least four times faster). Our baseline LR is not bad, but it is the worst in terms of metrics, which implies the non-linear nature of the problem. One of the factors that made us gave an special consideration to XGBoost compared to GBT is the execution time as XGBoost was faster than GBT.\n",
    "\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "- To obtain an optimal model, it is very important to do feature engineering, even with large amounts of data. Spark's machine learning (ML) library MLlib works faster with optimized dataframes.  \n",
    "\n",
    "- Downsampling and upsampling techniques were helpful to rebalance our training dataset.\n",
    "\n",
    "- With the large scale data, one major learning curve for the team was to start working with the 3m dataset and perform EDA, feature engineering and implementing the algorithms efficiently that could be easily reproduced for the larger dataset. \n",
    "\n",
    "- Writing to parquet files (creating and using checkpoints) after performing major joins/processing helped avoid re-running and spawning additional job tasks. \n",
    "\n",
    "- With the availability of a variety of algorithms PySpark, it was important to revisit concepts taught in the course to understand how and what algorithm would be appropriate and can be applied to solve a specific problem.\n",
    "\n",
    "**Lastly,**\n",
    "as we have seen flight departure delays depends on a variety of factors. In this project, we considered weather and few other factors. Flight delays are dependent on additional data points such as gate delays, flight crews availability, runway availability, equipment and other factors. These factors needs to be included in the research to achieve more accurate prediction of flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26ff2e6e-2175-4d8b-b11a-b41aa2beaf44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 7 -  Application of Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7157cb9f-2070-4907-a543-5d68cb805f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.1 One Hot Encoding / Vector embeddings / Feature selection\n",
    "\n",
    "One of the transformations we have applied in this project is One Hot Encoding. We have categorical features that do not have relationship with each other. This is because even though the feature consists of numerical values, there is no explicit ordering of the values. The various values mean different things, and one is not greater than or less than another. Because of this, we use one hot encoding for these features.\n",
    "\n",
    "Machine learning algorithms treat the order of numbers as an attribute of significance. In other words, they will read a higher number as better or more important than a lower number. While this is helpful for some ordinal situations, some input data does not have any ranking for category values, and this can lead to issues with predictions and poor performance. In scenarios like this, one hot encoding creates a binary column for categorical variable so that they can be used in algorithms like Logistic Regression, Support Vector Machines and others that cannot work directly with categorical data. In one hot encoding, categorical variables or features that do not have a numeric order are converted to numerical form by first assigning each category to an integer value. One hot encoding then converts each categorical value into a new categorical column and assigns a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1. This is shown in the example below.  \n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/one-hot.png' width=\"500\" height=\"700\">\n",
    "<br>\n",
    "\n",
    "One hot encoding is thus suited for both nominal and ordinal features and is a good method for encoding categorical features. It does not assume an inherent order in the categories. One hot encoding can be applied to categorical columns with low to medium cardinality. If a column has many unique values, this will result in sparse vectors for that feature. This may require more memory and compute time to process the algorithm. For columns with high cardinality, we can first group them into smaller groups. We can then performone hot encoding against these smaller groups. \n",
    "\n",
    "Vector encodings are list of numbers or vectors translated from numeric values, text documents etc., to vectors for easier operations and are suitable for ML taska such as clustering, classification and recommendation. The vector representation makes it possible to translate semantic similarity (metric defined over a set of documents or terms, where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to lexicographical similarity) as perceived by humans to proximity in a vector space. In a classification task, we classify the label of an unseen object by the major vote over labels of the most similar objects. Vector values can be engineered using domain knowledge for feature engineering and feature selection. However, this can be expensive to scale. \n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/vector_embeddings.jpg' width=\"700\" height=\"900\">\n",
    "<br>\n",
    "\n",
    "Feature selection is the process of reducing the number of input features when developing a ML model for prediction. This is needed to reduce the number of input features to both reduce the computational cost of modeling and improve performance. Features could be redundant and contain noise that may cause overfitting and negatively impact the performance of algorithms. Feature selection can be categorized into Filter, Wrapper and Embedded methods. The filter method is used applies statistical measure to the features independently from the prediction model and has a low computational cost and low risk of overfitting. The wrapper method uses repititive evaluation of subset combinations regarding the accuracy of the prediction model to find features and is computationally expensive and carries the risk of overfitting but has higher accuracy. The embedded method learns which feature subset has the best accuracy in creating time and suffers from overfitting. We have used the filter and wrapper methods for feature selection. \n",
    "\n",
    "<br>\n",
    "<img src ='https://sudhritybucket.s3.amazonaws.com/feature-selection.png' width=\"700\" height=\"900\">\n",
    "<br>\n",
    "\n",
    "\n",
    "References:<br>\n",
    "https://www.educative.io/blog/one-hot-encoding <br>\n",
    "https://www.pinecone.io/learn/vector-embeddings/ <br>\n",
    "https://en.wikipedia.org/wiki/Semantic_similarity <br>\n",
    "https://www.researchgate.net/publication/337591149_A_hybrid_feature-selection_approach_for_finding_the_digital_evidence_of_web_application_attacks#pf4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f8a1709-3d4a-406d-859c-61d175a60dd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.2 Lazy Evaluation, Caching, Broadcasting, Store results to Parquet for faster processing, DAGs \n",
    "With the knowledge of how PySpark's lazy evaluation works, we did not want previous instructions to be re-evaluated during our data analysis and feature engineering process when working with dataframes. We added cache() command where approriate to avoid re-evaluation and improve processing time. \n",
    "\n",
    "Spark can “broadcast” a small DataFrame by sending all the data in that small DataFrame to all nodes in the cluster. Spark can then perform a join without shuffling any of the data in the large DataFrame. This is one of the apporoches we have looked into to broadcast the results of joins for smaller datasets to improve efficiency. \n",
    "\n",
    "We also realized that writing intermediate results of computations including data from joins to parquet was useful to persist data to avoid data loss after 30 minutes of inactivity and also reading processed data from parquet instead of re-evaluating the data was more efficient. In addition, since our datasets are very large and often required aggregation queries (group by, sum etc.), we decided to store our data in columnar format (parquet) for better performance. The benefits of using Parquet for storage were:\n",
    "- Stored schema metadata at the end of the file for faster processing\n",
    "- Reduced storage requirements and provided higher throughput during ingestion and read. This was very helpful as the team needed to ingest multiple iterations of Parquet files during the feature engineering phases.\n",
    "- Faster processing could be achieved when aggregating queries in our project.\n",
    "- Allowed us to encode and store nested data types efficiently for sparsely populated data, which was the case for our datasets. \n",
    "\n",
    "Directed Acyclic Graph (DAG) in Spark is a set of Vertices and Edges, where vertices represent the RDDs and the edges represent the Operation to be applied on RDD. Actions trigger the scheduler, which builds a DAG, based on the dependencies between the RDD transformations. Spark keeps track of a DAG that contains the list of transformations needed to recreate each RDD and only do the transformations when an action is called. Directed Acyclic Graphs (DAGs) were reviewed to track the operations applied on the RDDs. DAG helps to achieve fault tolerance, thus we can recover the lost data. In addition, it can do a better global optimization than a system like MapReduce.\n",
    "\n",
    "Reference: <br>\n",
    "https://data-flair.training/blogs/dag-in-apache-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d7269c1-793c-420a-8611-43620b88cd71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.3 Spark Pipelines and Stages\n",
    "\n",
    "Spark Machine Learning workflow comprises a number of steps. This includes data ingesting, cleaning, preprocessing, encoding, modeling etc. When dealing with large amounts of data as is the case with this project, this can be an issue. Spark's MLlib provides variety of features including transformers and estimators that can be included as wrappers in the worflow pipeline to execute ML models. \n",
    "\n",
    "In our pipelines, we have used multiple transformation stages, such as label indexer, string indexer, one hot encoder, and standard scaler. The label indexer is used to transform the prediction label (dep_delayed). The string indexer and one hot encoders are used for categorical variables. The standard scaler is used to normalize inputs to zero mean and unit variance. After performing the transformations, we have added the estimators/classifiers and these are executed when an 'action' is invoked based on Spark's lazy evaluation mechanism. Spark uses optimized DAGs to streamline the execution of the pipeline stages. Caching at different stages of the pipeline helped optimize execution performance.\n",
    "\n",
    "We architected the implementation to reuse the pipeline stages that resulted in a plug-n-play approach for modeling for the algorithms that we used in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0cf2840d-26eb-4a36-8597-5a73d08ef7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.4 Scalability / Time complexity / I/O vs Memory:\n",
    "\n",
    "The dataset for the project being quite large, we were limited with design and implement choices. Also, due to the imbalance in the dataset, we noticed the training set was causing the model to predict negative (on-time departure) too often. We downsampled the negatives to equal the positive (delayed flights) count on the training sets only, and this improved results on the validation/test sets.\n",
    "\n",
    "Using pandas/toDF() methods resulted in bottleneck issues for the driver. For processing large amounts of data, the time complexity increased exponentially. We used flags to prevent using pandas/toDF() while processing full dataset.\n",
    "\n",
    "Data processing in-memory was faster but was limited to the processing power of the small cluster. File I/O while writing data to parquet was quite slow for large data and took hours to complete. We had to reduce columns (for column oriented storage) prior to writing to parquet. We attempted to write outcome of data processing (e.g. joined data) to parquet to avoid duplicate processing of the data. This helped us store intermediate results in our pipeline to disk and avoid dataloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "519f94f6-6078-4d13-a834-70b6b12092bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 7.5 Normalization:\n",
    "\n",
    "Feature scaling through normalization is an important step for many machine learning algorithms such as SVM, k-nearest neighbors, and logistic regression. This process involves rescaling the features so they have a normal distribution with a mean of zero and a standard deviation of one. Principal Component Analysis (PCA) is one of the prime examples of when normalization is important. PCA is affected by scale since we are interested in the components that maximaze the variance. There are many ways to normalize data, such as min-max scaling, which maps every feature to be between 0 and 1, as well as Z-score standardization, which transforms the distribution to be \\\\(~N(0,1)\\\\).\n",
    "\n",
    "In our project, we used StandardScaler to standarize the dataset's features before applying PCA. StandardScaler was applied to the \"numeric_vec\" column to generate the \"scaled_features\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d428967b-a641-434b-b4db-79f447f6d35f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[1] Flight on-time performance data from TranStats data collection, U.S. Department of Transportation. https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time <br>\n",
    "[2] Cost delay estimates 2019. Federal Aviation Agency https://www.faa.gov/data_research/aviation_data_statistics/media/cost_delay_estimates.pdf <br>\n",
    "[3] Chakrabarty, N., (2019), “A data mining approach to flight arrival delay prediction for American airlines”, At 9th Annual Information Technology Electromechanical Engineering and Microelectronics Conference, Jaipur, India.<br>\n",
    "[4] Belcastro, L., Marozzo, F., Talia, D., & Trunfio, P. (2016). Using Scalable Data Mining for Predicting Flight DelaysACM Trans. Intell. Syst. Technol., 8(1).<br>\n",
    "[5] S. Choi, Y. J. Kim, S. Briceno and D. Mavris, \"Prediction of weather-induced airline delays based on machine learning algorithms,\" 2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC), Sacramento, CA, 2016, pp. 1-6, doi: 10.1109/DASC.2016.7777956.<br>\n",
    "[6] Y. Jiang, Y. Liu, D. Liu and H. Song, \"Applying Machine Learning to Aviation Big Data for Flight Delay Prediction,\" 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), Calgary, AB, Canada, 2020, pp. 665-672, doi: 10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114.<br>\n",
    "[7] https://www.airlines.org/dataset/u-s-passenger-carrier-delay-costs/<br>\n",
    "[8] https://www.faa.gov/nextgen/programs/weather/faq/<br>\n",
    "[9] Yazdi, M.F., Kamel, S.R., Chabok, S.J.M. et al. Flight delay prediction based on deep learning and Levenberg-Marquart algorithm. J Big Data 7, 106 (2020) (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00380-z)<br>\n",
    "[10] Huo, Jiage & Keung, K.L. & Lee, C. & Ng, Kam K.H. & Li, K.C.. The Prediction of Flight Delay: Big Data-driven Machine Learning Approach. (2021). 10.1109/IEEM45057.2020.9309919<br>\n",
    "[11] Weather station records, National Oceanic and Atmospheric Administration, https://www.ncdc.noaa.gov/orders/qclcd/ <br>\n",
    "[12] https://openflights.org/data.html <br>\n",
    "[13] https://medium.com/analytics-vidhya/modeling-flight-delays-through-u-s-flight-data-2f0b3d7e2c89 <br>\n",
    "[14] https://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf <br>\n",
    "[15] https://www.hindawi.com/journals/jat/2021/4292778/ <br>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "W261_FA21_FINAL_PROJECT_TEAM11_NB1_Overview",
   "notebookOrigID": 2661063350709337,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
