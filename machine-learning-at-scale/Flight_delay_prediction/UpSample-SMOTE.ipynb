{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "683d4c13-182d-4f4c-93de-0d6d7833a480",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Implementing Oversampling technique (SMOTE)\n",
    "\n",
    "**This notebook is to demonstrate oversampling technique applied to handle imbalanced dataset.**\n",
    "\n",
    "The resulting dataframe that came out of the SMOTE implementation was then applied to 2 models - Logistic regression and Random Forest. However, due to the limited cluster memory availability, both model run could not succeed and aborted with this error message - \"**Error: java.lang.OutOfMemoryError: GC overhead limit exceeded**\"\n",
    "\n",
    "We are attaching this notebook just as a reference to showcase the implementation logic for SMOTE technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a7aeb2f-4fb3-4f9b-bb04-a8b1b9514a13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 1 - Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "860d1e18-8843-40ec-9b06-baca76a0b902",
     "showTitle": true,
     "title": "Import Libraries"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from functools import reduce\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import rand,udf,lower,sum as ps_sum,count as ps_count,row_number\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import VectorAssembler,BucketedRandomProjectionLSH,VectorSlicer\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "from pyspark.sql.functions import array, create_map, struct\n",
    "\n",
    "from pyspark.sql.functions import col, concat, count, countDistinct, expr, lit, length, max as pyspark_max, min as pyspark_min, mean, substring, split, sum as pyspark_sum, when, to_utc_timestamp, to_timestamp, unix_timestamp, isnan\n",
    "import pandas as pd\n",
    "from html import escape\n",
    "from IPython.display import HTML, display as ipython_display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import IntegerType, StringType, BooleanType, DateType, DoubleType\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, max, substring\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.classification import LinearSVC as svc\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c1eb45d1-01f5-4e23-9333-2ccc83e93020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "blob_container = \"w261-container\" # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"w261storageaccount\" # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope = \"w261scope\" # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key = \"w261key\" # The name of the secret key created in your local computer using the Databricks CLI \n",
    "blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\n",
    "mount_path = \"/mnt/mids-w261\"\n",
    "\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fdcf5435-75c1-4660-84a3-f3da50d7c54a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 2 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ea0bb1d-c08a-4522-9ff8-4c9e5c20a6a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the split test and train data from blob storage in case of full data\n",
    "# Reading the toy test and train data from blob storage in case of toy dataset\n",
    "\n",
    "train_df = spark.read.parquet(f\"{blob_url}/original_train_data_full_v2/*\")\n",
    "test_df = spark.read.parquet(f\"{blob_url}/original_test_data_full_v2/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e8b85094-2716-4928-9144-e6663a9a75d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\n",
       "----Categorical columns train-----\n",
       "[&#39;year&#39;, &#39;month&#39;, &#39;day_of_week&#39;, &#39;hour&#39;, &#39;carrier&#39;, &#39;holiday&#39;, &#39;weather_obs_prev&#39;, &#39;weather_obs_prev_2&#39;, &#39;origin&#39;, &#39;destination&#39;, &#39;prev_dep_delayed_confirmed&#39;, &#39;wnd_type_prev&#39;, &#39;vis_var_prev&#39;]\n",
       "\n",
       "----Numeric columns train-----\n",
       "[&#39;planned_time_between_flights&#39;, &#39;distance&#39;, &#39;wnd_angle_prev&#39;, &#39;wnd_speed_prev_2&#39;, &#39;wnd_speed_prev&#39;, &#39;wnd_cloud_angle_prev&#39;, &#39;vis_dist_prev&#39;, &#39;vis_dist_prev_2&#39;, &#39;tmp_c_prev&#39;, &#39;dew_c_prev&#39;, &#39;route_delay&#39;, &#39;origin_arrival_delay&#39;, &#39;origin_departure_delay&#39;, &#39;dest_departure_delay&#39;, &#39;dest_arrival_delay&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">\n----Categorical columns train-----\n[&#39;year&#39;, &#39;month&#39;, &#39;day_of_week&#39;, &#39;hour&#39;, &#39;carrier&#39;, &#39;holiday&#39;, &#39;weather_obs_prev&#39;, &#39;weather_obs_prev_2&#39;, &#39;origin&#39;, &#39;destination&#39;, &#39;prev_dep_delayed_confirmed&#39;, &#39;wnd_type_prev&#39;, &#39;vis_var_prev&#39;]\n\n----Numeric columns train-----\n[&#39;planned_time_between_flights&#39;, &#39;distance&#39;, &#39;wnd_angle_prev&#39;, &#39;wnd_speed_prev_2&#39;, &#39;wnd_speed_prev&#39;, &#39;wnd_cloud_angle_prev&#39;, &#39;vis_dist_prev&#39;, &#39;vis_dist_prev_2&#39;, &#39;tmp_c_prev&#39;, &#39;dew_c_prev&#39;, &#39;route_delay&#39;, &#39;origin_arrival_delay&#39;, &#39;origin_departure_delay&#39;, &#39;dest_departure_delay&#39;, &#39;dest_arrival_delay&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preparing final dataset for modeling\n",
    "# Let's find out the datatype of the features, which will be the same for both train and test data\n",
    "\n",
    "print(\"\\n----Categorical columns train-----\")\n",
    "cat_cols = [x for (x, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "print(cat_cols)\n",
    "\n",
    "print(\"\\n----Numeric columns train-----\")\n",
    "num_cols = [x for (x, dataType) in train_df.dtypes if (((dataType == \"double\") or (dataType == \"int\")) & (x != \"departure_delay_boolean\"))]\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a83eb558-de56-49a1-a0af-ba2b9e8783fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One hot encoding - converting categorical columns into binary sparse vectors\n",
    "\n",
    "from pyspark.ml.feature import (OneHotEncoder, StringIndexer)\n",
    "string_indexer = [\n",
    "    StringIndexer(inputCol=x, outputCol=x + \"_StringIndexer\", handleInvalid=\"skip\")\n",
    "    for x in cat_cols\n",
    "]\n",
    "\n",
    "one_hot_encoder = [\n",
    "    OneHotEncoder(\n",
    "        inputCols=[f\"{x}_StringIndexer\" for x in cat_cols],\n",
    "        outputCols=[f\"{x}_OneHotEncoder\" for x in cat_cols],\n",
    "    )\n",
    "]\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = [\n",
    "      Imputer(inputCol=x, outputCol= x + \"_Imputed\").setStrategy(\"median\")\n",
    "      for x in num_cols\n",
    "]\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler  \n",
    "\n",
    "scaler_vector_assembler = VectorAssembler(inputCols = [f\"{x}_Imputed\" for x in num_cols], outputCol=\"numeric_vec\")\n",
    "scaler_vector_assembler.setHandleInvalid('keep')\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"numeric_vec\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0aa53d35-6834-4bab-88a8-8ac8032c1883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler_input = [\"scaled_features\"]\n",
    "assembler_input += [f\"{x}_OneHotEncoder\" for x in cat_cols]\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=assembler_input, outputCol=\"VectorAssembler_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b471c6d-8538-436c-8c59-adef85cf957c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 3 - Imbalanced data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b48dce3-93d4-417f-89fd-b80c59da96fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.0.  Setting up Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "183ca5e1-60ed-417e-921e-a189f567d94a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stages = []\n",
    "stages += imputer\n",
    "stages += [scaler_vector_assembler , scaler]\n",
    "stages += string_indexer\n",
    "stages += one_hot_encoder\n",
    "stages += [vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9bab2ffb-4c28-49e8-8023-35174d36ee23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "general_pipeline = Pipeline().setStages(stages)\n",
    "pipeline_model = general_pipeline.fit(train_df)\n",
    "model_data_test = pipeline_model.transform(test_df)\n",
    "model_data_train = pipeline_model.transform(train_df) \n",
    "\n",
    "selectedcols = [\"departure_delay_boolean\", \"VectorAssembler_features\"]\n",
    "model_data_train = model_data_train.select(selectedcols)\n",
    "model_data_test = model_data_test.select(selectedcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f4d21b3-cee6-426f-9deb-1e19fe36e5ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_data_train.write.mode('overwrite').parquet(f\"{blob_url}/train_data_pre_upsample\")\n",
    "model_data_test.write.mode('overwrite').parquet(f\"{blob_url}/test_data_pre_upsample\")\n",
    "train_df = spark.read.parquet(f\"{blob_url}/train_data_pre_upsample/*\")\n",
    "test_df = spark.read.parquet(f\"{blob_url}/test_data_pre_upsample/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "507e09c4-c37b-4706-896b-ba8fe25b6374",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3.1. Oversampling (SMOTE)\n",
    "SMOTE is a method to handle imbalanced data and is applied to upsample the minority class dataset by creating synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "afaf7a92-f418-4957-932b-b444f411e756",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SMOTE Implementation for handling imbalanced data between majority and minority class\n",
    "\n",
    "def smote(df):\n",
    "    '''\n",
    "    contains logic to perform smote oversampling, given a spark df with 2 classes\n",
    "    inputs:\n",
    "    * df: cat cols are already stringindexed, num cols are assembled into 'VectorAssembler_features' vector\n",
    "      df target col should be 'departure_delay_boolean'\n",
    "    output:\n",
    "    * oversampled_df: spark df after smote oversampling\n",
    "    '''\n",
    "    dataInput_min = df[df['departure_delay_boolean'] == 1]\n",
    "    dataInput_maj = df[df['departure_delay_boolean'] == 0]\n",
    "    \n",
    "    # seed = Random, 12345 or 40\n",
    "    # bucketLength = math.pow(numRecords, -1/inputDim) = math.pow(5017444, -1/26)\n",
    "     # LSH, bucketed random projection\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"VectorAssembler_features\", outputCol=\"hashes\",seed=12345, bucketLength=0.55)\n",
    "    # smote only applies on existing minority instances    \n",
    "    model = brp.fit(dataInput_min)\n",
    "    model.transform(dataInput_min)\n",
    "\n",
    "    # here distance is calculated from brp's param inputCol\n",
    "    self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n",
    "\n",
    "    # remove self-comparison (distance 0)\n",
    "    self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n",
    "\n",
    "    over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
    "\n",
    "    self_similarity_df = self_join_w_distance.withColumn(\"r_num\", F.row_number().over(over_original_rows))\n",
    "\n",
    "    self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= 2)\n",
    "\n",
    "    over_original_rows_no_order = Window.partitionBy('datasetA')\n",
    "\n",
    "    # list to store batches of synthetic data\n",
    "    res = []\n",
    "    \n",
    "    # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
    "    subtract_vector_udf = F.udf(lambda arr: random.uniform(0, 1)*(arr[0]-arr[1]), VectorUDT())\n",
    "    add_vector_udf = F.udf(lambda arr: arr[0]+arr[1], VectorUDT())\n",
    "    \n",
    "    # retain original columns\n",
    "    original_cols = dataInput_min.columns\n",
    "    # range_multiplier = dataInput_maj / dataInput_min\n",
    "    range_multiplier = 10\n",
    "    for i in range(range_multiplier):\n",
    "        print(\"generating batch %s of synthetic instances\"%i)\n",
    "        # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
    "        df_random_sel = self_similarity_df_selected.withColumn(\"rand\", F.rand()).withColumn('max_rand', F.max('rand').over(over_original_rows_no_order))\\\n",
    "                            .where(F.col('rand') == F.col('max_rand')).drop(*['max_rand','rand','r_num'])\n",
    "        # create synthetic feature numerical part\n",
    "        df_vec_diff = df_random_sel.select('*', subtract_vector_udf(F.array('datasetA.VectorAssembler_features', 'datasetB.VectorAssembler_features')).alias('vec_diff'))\n",
    "        df_vec_modified = df_vec_diff.select('*', add_vector_udf(F.array('datasetA.VectorAssembler_features', 'vec_diff')).alias('VectorAssembler_features'))\n",
    "        \n",
    "        # for categorical cols, either pick original or the neighbour's cat values\n",
    "        for c in original_cols:\n",
    "            # randomly select neighbour or original data\n",
    "            col_sub = random.choice(['datasetA','datasetB'])\n",
    "            val = \"{0}.{1}\".format(col_sub,c)\n",
    "            if c != 'VectorAssembler_features':\n",
    "                # do not unpack original numerical features\n",
    "                df_vec_modified = df_vec_modified.withColumn(c,F.col(val))\n",
    "        \n",
    "        # this df_vec_modified is the synthetic minority instances,\n",
    "        df_vec_modified = df_vec_modified.drop(*['datasetA','datasetB','vec_diff','EuclideanDistance'])\n",
    "        \n",
    "        res.append(df_vec_modified)\n",
    "    \n",
    "    dfunion = reduce(DataFrame.unionAll, res)\n",
    "    # union synthetic instances with original full (both minority and majority) df\n",
    "    oversampled_df = dfunion.union(df.select(dfunion.columns))\n",
    "    \n",
    "    return oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6badb7b8-495b-49c2-bd84-7bf1eacd915f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">generating batch 0 of synthetic instances\n",
       "generating batch 1 of synthetic instances\n",
       "generating batch 2 of synthetic instances\n",
       "generating batch 3 of synthetic instances\n",
       "generating batch 4 of synthetic instances\n",
       "generating batch 5 of synthetic instances\n",
       "generating batch 6 of synthetic instances\n",
       "generating batch 7 of synthetic instances\n",
       "generating batch 8 of synthetic instances\n",
       "generating batch 9 of synthetic instances\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">generating batch 0 of synthetic instances\ngenerating batch 1 of synthetic instances\ngenerating batch 2 of synthetic instances\ngenerating batch 3 of synthetic instances\ngenerating batch 4 of synthetic instances\ngenerating batch 5 of synthetic instances\ngenerating batch 6 of synthetic instances\ngenerating batch 7 of synthetic instances\ngenerating batch 8 of synthetic instances\ngenerating batch 9 of synthetic instances\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "oversampled_train_df = smote(model_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65409c26-83ac-4bd1-bb25-1b573289eb85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-2297543790346889&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>oversampled_train_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;overwrite&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">f&#34;{blob_url}/train_data_upsample&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> train_df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">f&#34;{blob_url}/train_data_upsample/*&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, path, mode, partitionBy, compression)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1275</span>             self<span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span>partitionBy<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1276</span>         self<span class=\"ansi-blue-fg\">.</span>_set_opts<span class=\"ansi-blue-fg\">(</span>compression<span class=\"ansi-blue-fg\">=</span>compression<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1277</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1278</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1279</span>     <span class=\"ansi-green-fg\">def</span> text<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> compression<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span> lineSep<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o6374.parquet.\n",
       ": org.apache.spark.SparkException: Job aborted.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:307)\n",
       "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:194)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:121)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:119)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:144)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:257)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:253)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1079)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1079)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:468)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:438)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:303)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:964)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 137792.0 failed 4 times, most recent failure: Lost task 7.3 in stage 137792.0 (TID 690474) (10.0.128.119 executor 150): ExecutorLostFailure (executor 150 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2297543790346889&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>oversampled_train_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;overwrite&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">f&#34;{blob_url}/train_data_upsample&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> train_df <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">f&#34;{blob_url}/train_data_upsample/*&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, path, mode, partitionBy, compression)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1275</span>             self<span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span>partitionBy<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1276</span>         self<span class=\"ansi-blue-fg\">.</span>_set_opts<span class=\"ansi-blue-fg\">(</span>compression<span class=\"ansi-blue-fg\">=</span>compression<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1277</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1278</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1279</span>     <span class=\"ansi-green-fg\">def</span> text<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> compression<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span> lineSep<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o6374.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:194)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:121)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:119)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:257)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1079)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:130)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:273)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:223)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1079)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:468)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:303)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:964)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 137792.0 failed 4 times, most recent failure: Lost task 7.3 in stage 137792.0 (TID 690474) (10.0.128.119 executor 150): ExecutorLostFailure (executor 150 exited caused by one of the running tasks) Reason: Command exited with code 52\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n</div>",
       "errorSummary": "org.apache.spark.SparkException: Job aborted.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "oversampled_train_df.write.mode('overwrite').parquet(f\"{blob_url}/train_data_upsample\")\n",
    "train_df = spark.read.parquet(f\"{blob_url}/train_data_upsample/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb336d07-a2cf-41f5-ac79-0eea369eeb3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Section 4 - ML section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9605978-59c9-4660-b0d0-064a67af5a3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4.0. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "88e94f08-5720-4d55-9b96-fab57db9bfeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-671119174920103&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> stages_lr <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>lr<span class=\"ansi-blue-fg\">]</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> pipeline_lr <span class=\"ansi-blue-fg\">=</span> Pipeline<span class=\"ansi-blue-fg\">(</span>stages<span class=\"ansi-blue-fg\">=</span>stages_lr<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 16</span><span class=\"ansi-red-fg\"> </span>lr_model <span class=\"ansi-blue-fg\">=</span> pipeline_lr<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>train_df<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     17</span> lrPredictions <span class=\"ansi-blue-fg\">=</span> lr_model<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>test_df<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     18</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/pipeline.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>                     dataset <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    113</span>                 <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>  <span class=\"ansi-red-fg\"># must be an Estimator</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 114</span><span class=\"ansi-red-fg\">                     </span>model <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                     transformers<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>                     <span class=\"ansi-green-fg\">if</span> i <span class=\"ansi-blue-fg\">&lt;</span> indexOfLastEstimator<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 335</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    337</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>         &#34;&#34;&#34;\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    331</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 332</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o8348.fit.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 137806.0 failed 4 times, most recent failure: Lost task 4.3 in stage 137806.0 (TID 690574) (10.0.128.119 executor 155): scala.MatchError: [null,1.0,(804,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,33,40,73,77,78,79,98,494,797,799,802],[0.01227876010576003,0.4919111581895064,1.8347581029801274,2.5014659186006987,2.709770520485189,0.16987135815752624,4.853988524506112,4.830009665721882,2.028271771493124,1.7381782712661273,3.938640625574508,6.17684452651295,6.3059605298552395,5.114406777003982,4.582645043415024,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
       "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
       "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
       "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
       "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
       "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1240)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1241)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:868)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:868)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2476)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2459)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2571)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1193)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n",
       "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1187)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1256)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n",
       "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1232)\n",
       "\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:232)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:513)\n",
       "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:288)\n",
       "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
       "\tat sun.reflect.GeneratedMethodAccessor930.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: scala.MatchError: [null,1.0,(804,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,33,40,73,77,78,79,98,494,797,799,802],[0.01227876010576003,0.4919111581895064,1.8347581029801274,2.5014659186006987,2.709770520485189,0.16987135815752624,4.853988524506112,4.830009665721882,2.028271771493124,1.7381782712661273,3.938640625574508,6.17684452651295,6.3059605298552395,5.114406777003982,4.582645043415024,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
       "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
       "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n",
       "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n",
       "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n",
       "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1240)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1241)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:868)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:868)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-671119174920103&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> stages_lr <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>lr<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span> pipeline_lr <span class=\"ansi-blue-fg\">=</span> Pipeline<span class=\"ansi-blue-fg\">(</span>stages<span class=\"ansi-blue-fg\">=</span>stages_lr<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 16</span><span class=\"ansi-red-fg\"> </span>lr_model <span class=\"ansi-blue-fg\">=</span> pipeline_lr<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>train_df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     17</span> lrPredictions <span class=\"ansi-blue-fg\">=</span> lr_model<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>test_df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span> \n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/pipeline.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>                     dataset <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    113</span>                 <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>  <span class=\"ansi-red-fg\"># must be an Estimator</span>\n<span class=\"ansi-green-fg\">--&gt; 114</span><span class=\"ansi-red-fg\">                     </span>model <span class=\"ansi-blue-fg\">=</span> stage<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                     transformers<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>                     <span class=\"ansi-green-fg\">if</span> i <span class=\"ansi-blue-fg\">&lt;</span> indexOfLastEstimator<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 335</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    336</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    337</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    331</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 332</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    333</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o8348.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 137806.0 failed 4 times, most recent failure: Lost task 4.3 in stage 137806.0 (TID 690574) (10.0.128.119 executor 155): scala.MatchError: [null,1.0,(804,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,33,40,73,77,78,79,98,494,797,799,802],[0.01227876010576003,0.4919111581895064,1.8347581029801274,2.5014659186006987,2.709770520485189,0.16987135815752624,4.853988524506112,4.830009665721882,2.028271771493124,1.7381782712661273,3.938640625574508,6.17684452651295,6.3059605298552395,5.114406777003982,4.582645043415024,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1240)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1241)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:868)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:868)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2476)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2459)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2571)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1193)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1187)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1232)\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:232)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:513)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:288)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat sun.reflect.GeneratedMethodAccessor930.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: scala.MatchError: [null,1.0,(804,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,33,40,73,77,78,79,98,494,797,799,802],[0.01227876010576003,0.4919111581895064,1.8347581029801274,2.5014659186006987,2.709770520485189,0.16987135815752624,4.853988524506112,4.830009665721882,2.028271771493124,1.7381782712661273,3.938640625574508,6.17684452651295,6.3059605298552395,5.114406777003982,4.582645043415024,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1240)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1241)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:868)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:868)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:380)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:344)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 137806.0 failed 4 times, most recent failure: Lost task 4.3 in stage 137806.0 (TID 690574) (10.0.128.119 executor 155): scala.MatchError: [null,1.0,(804,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,23,33,40,73,77,78,79,98,494,797,799,802],[0.01227876010576003,0.4919111581895064,1.8347581029801274,2.5014659186006987,2.709770520485189,0.16987135815752624,4.853988524506112,4.830009665721882,2.028271771493124,1.7381782712661273,3.938640625574508,6.17684452651295,6.3059605298552395,5.114406777003982,4.582645043415024,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Setup pipeline\n",
    "\n",
    "start_lr = time.time()\n",
    "\n",
    "lr = LogisticRegression(featuresCol='VectorAssembler_features',\n",
    "                        labelCol='departure_delay_boolean', \n",
    "                        regParam = 0.2, \n",
    "                        fitIntercept=True, \n",
    "                        elasticNetParam=0.0, \n",
    "                        maxIter=10, \n",
    "                        threshold = 0.6)\n",
    "\n",
    "stages_lr = [lr]\n",
    "pipeline_lr = Pipeline(stages=stages_lr)\n",
    "lr_model = pipeline_lr.fit(train_df)\n",
    "lrPredictions = lr_model.transform(test_df)\n",
    "\n",
    "lrPredictions.write.mode('overwrite').parquet(f\"{blob_url}/lrPredictions_upsample\")\n",
    "\n",
    "print(f'lr model completed job in {time.time() - start_lr} seconds.')\n",
    "lr_time_model = time.time() - start_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c20dfb1-12de-4d59-b169-7c401ef6831d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-671119174920105&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Logistic Regression Model Evaluation with scikit learn</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n",
       "<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>lrPredictions <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">f&#34;{blob_url}/lrPredictions_upsample/*&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> y_true_lr <span class=\"ansi-blue-fg\">=</span> lrPredictions<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;departure_delay_boolean&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, *paths, **options)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    483</span>                        int96RebaseMode=int96RebaseMode)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    484</span> \n",
       "<span class=\"ansi-green-fg\">--&gt; 485</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>_to_seq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    486</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    487</span>     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: wasbs://w261-container@w261storageaccount.blob.core.windows.net/lrPredictions_upsample/*</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-671119174920105&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Logistic Regression Model Evaluation with scikit learn</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>lrPredictions <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">f&#34;{blob_url}/lrPredictions_upsample/*&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> y_true_lr <span class=\"ansi-blue-fg\">=</span> lrPredictions<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;departure_delay_boolean&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, *paths, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    483</span>                        int96RebaseMode=int96RebaseMode)\n<span class=\"ansi-green-intense-fg ansi-bold\">    484</span> \n<span class=\"ansi-green-fg\">--&gt; 485</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>_to_seq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    487</span>     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: wasbs://w261-container@w261storageaccount.blob.core.windows.net/lrPredictions_upsample/*</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AnalysisException</span>: Path does not exist: wasbs://w261-container@w261storageaccount.blob.core.windows.net/lrPredictions_upsample/*",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression Model Evaluation with scikit learn\n",
    "\n",
    "lrPredictions = spark.read.parquet(f\"{blob_url}/lrPredictions_upsample/*\")\n",
    "\n",
    "y_true_lr = lrPredictions.select(['departure_delay_boolean']).collect()\n",
    "y_pred_lr = lrPredictions.select(['prediction']).collect()\n",
    "\n",
    "# Print metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true_lr, y_pred_lr))\n",
    "print(confusion_matrix(y_true_lr, y_pred_lr))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "W261_FA21_FINAL_PROJECT_TEAM11_NB8_Algorithm_Exploration_UpSample",
   "notebookOrigID": 671119174920056,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
